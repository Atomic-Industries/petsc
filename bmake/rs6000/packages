# $Id: base.site,v 1.37 2000/09/13 15:17:59 balay Exp bsmith $ 

#
#  This file contains site-specific information.  The definitions below
#  should be changed to match the locations of libraries at your site.
#  The following naming convention is used:
#     XXX_LIB - location of library XXX
#     XXX_INCLUDE - directory for include files needed for library XXX
#
# Location of BLAS and LAPACK.  See ${PETSC_DIR}/docs/installation.html
# for information on retrieving them.
# Important: If you use the IBM version of lapack you must include 
#  -lessl at the end of the line defining the BLAS libraries.
#
BLAS_LIB       = -lblas
#LAPACK_LIB     = -lessl /usr/local/lib/liblapack.a
LAPACK_LIB     = -lessl /home/petsc/software/blaslapack/lapack_rs6000_p4.a
#
# Location of MPI (Message Passing Interface) software
#
#MPI_LIB        = ${PETSC_DIR}/lib/lib${BOPT}/${PETSC_ARCH}/libmpiuni.a
#MPI_INCLUDE    = -I${PETSC_DIR}/src/sys/src/mpiuni -DPETSC_HAVE_INT_MPI_COMM
#MPIRUN         = ${PETSC_DIR}/src/sys/src/mpiuni/mpirun

#MPI_LIB        = -L/home/petsc/mpich-1.1.1/lib/rs6000/ch_p4 -lmpich
#MPI_INCLUDE    = -I/home/petsc/mpich-1.1.1/include -I/home/petsc/mpich-1.1.1/lib/rs6000/ch_p4
#MPIRUN         = /home/petsc/mpich-1.1.1/lib/rs6000/ch_p4/mpirun -machinefile /home/bsmith/petsc/maint/hosts.local
# -lbsd causes to hang. Hence taking it out.
MPI_HOME       = /home/petsc/software/mpich-1.1.2
MPI_BUILD_HOME = ${MPI_HOME}/build/rs6000/ch_p4
MPI_LIB        = -L${MPI_BUILD_HOME}/lib -lpmpich -lmpich
MPI_INCLUDE    = -I${MPI_HOME}/include -I${MPI_BUILD_HOME}/include
MPIRUN         = ${MPI_BUILD_HOME}/bin/mpirun -machinefile /home/bsmith/petsc/maint/hosts.local

#
# ----------------------------------------------------------------------------------------  
#  Locations of OPTIONAL packages. Comment out those you do not have.
# ----------------------------------------------------------------------------------------  
#
# Location of X-windows software
#
X11_INCLUDE    = 
X11_LIB        = -lX11
PETSC_HAVE_X11 = -DPETSC_HAVE_X11
#
# Location of BlockSolve (MPI version), if you do not have
# BlockSolve, then remove -DPETSC_HAVE_BLOCKSOLVE from the PCONF definition
# below if it is there.
#
#BLOCKSOLVE_INCLUDE = -I/home/petsc/BlockSolve95/include
#BLOCKSOLVE_LIB     = -L/home/petsc/BlockSolve95/lib/libO/${PETSC_ARCH} -lBS95
#PETSC_HAVE_BLOCKSOLVE = -DPETSC_HAVE_BLOCKSOLVE
#
# MPE Libraries
# If using MPICH version 1.1.2 or higher use the flag -DPETSC_HAVE_MPE_INITIALIZED_LOGGING
#
#MPE_INCLUDE  = -I/home/petsc/mpich-1.1.1/mpe
#MPE_LIB      = -lmpe -lpmpich
MPE_INCLUDE   = -I${MPI_HOME}/mpe 
MPE_LIB       = -L${MPI_BUILD_HOME}/lib -lmpe
PETSC_HAVE_MPE = -DPETSC_HAVE_MPE
#
# Matlab location, remove if not needed.
#
#CMEX            = 
#MCC            = 
#MATLABCOMMAND  = matlab
#PETSC_HAVE_MATLAB =  -DPETSC_HAVE_MATLAB
#
#
#SUPERLU_LIB     = /home/bsmith/SuperLU/superlu${PETSC_ARCH}${BOPT}.a
#SUPERLU_INCLUDE = -I/home/bsmith/SuperLU/SRC
#PETSC_HAVE_SUPERLU = -DPETSC_HAVE_SUPERLU 
#
# Location of the LUSOL sparse LU factorization code (part of MINOS)
# developed by Michael Saunders, saunders@stanford.edu at the
# Systems Optimization Laboratory, Stanford University.
#  http://www.sbsi-sol-optimize.com/
#
#
# Put the two files mi25bfac.f and mi15blas.f (or LUSOL.f LUSOL_BLAS.f
# depending on how they are named) into src/mat/impls/aij/seq and 
# uncomment the appropriate lines below
#
PETSC_HAVE_LUSOL     = -DPETSC_HAVE_LUSOL
#PETSC_LUSOL_SOURCE_F = LUSOL.f LUSOL_BLAS.f
#PETSC_LUSOL_SOURCE_O = LUSOL.o LUSOL_BLAS.o
PETSC_LUSOL_SOURCE_F = mi25bfac.f mi15blas.f
PETSC_LUSOL_SOURCE_O = mi25bfac.o mi15blas.o




