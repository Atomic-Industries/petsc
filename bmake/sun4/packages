# $Id: base.site,v 1.58 1999/11/05 14:42:55 bsmith Exp bsmith $ 
#
#
#  This file contains site-specific information.  The definitions below
#  should be changed to match the locations of libraries at your site.
#  The following naming convention is used:
#     XXX_LIB - location of library XXX
#     XXX_INCLUDE - directory for include files needed for library XXX
#
# FC_LIB contains the libraries for a CLINKER to use for code containing
# FORTRAN code (such as I/O or special Fortran runtimes).  
#
#
# Location of BLAS and LAPACK.  See ${PETSC_DIR}/docs/installation.html 
# for information on retrieving them.
#
# If your machine has the file /usr/lang/SC3.0.1/lib/libsunperf.a then
# you can use -lsunperf or /usr/lang/SC3.0.1/lib/libsunperf.a below as the 
# BLAS and LAPACK library
#
BLAS_LIB       =  /usr/local/lapack/lib/blas.a
LAPACK_LIB     =  /usr/local/lapack/lib/lapack.a

#
# Location of MPI (Message Passing Interface) software  
#
#MPI_LIB        = -L/usr/local/mpi/lib/sun4/ch_p4 -lmpi
#MPI_INCLUDE    = -I/usr/local/mpi/include
#MPIRUN         = /usr/local/mpi/bin/mpirun -machinefile /home/bsmith/petsc/maint/hosts.local
MPI_HOME       = /home/petsc/software/mpich-1.1.2
MPI_BUILD_HOME = ${MPI_HOME}/build/sun4/ch_p4
MPI_LIB        = -L${MPI_BUILD_HOME}/lib -lpmpich -lmpich  \
		-L/usr/local/gcc-2.7.2/lib/gcc-lib/sparc-sun-sunos4.1.4/2.7.2 -lgcc
MPI_INCLUDE    = -I${MPI_HOME}/include -I${MPI_BUILD_HOME}/include
MPIRUN         = ${MPI_BUILD_HOME}/bin/mpirun -machinefile /home/bsmith/petsc/maint/hosts.local

#
# ----------------------------------------------------------------------------------------  
#  Locations of OPTIONAL packages. Comment out those you do not have.
# ----------------------------------------------------------------------------------------  
#
# Location of X-windows software
#
X11_INCLUDE    = -I/usr/local/X11R5/include
X11_LIB        = -lX11
PETSC_HAVE_X11 = -DPETSC_HAVE_X11
#
# Location of MPE
# If using MPICH version 1.1.2 or higher use the flag -DPETSC_HAVE_MPE_INITIALIZED_LOGGING
#
MPE_INCLUDE   = -I${MPI_HOME}/mpe 
MPE_LIB       = -L${MPI_BUILD_HOME}/lib -lmpe
PETSC_HAVE_MPE = -DPETSC_HAVE_MPE
#
# Location of BlockSolve (MPI version)
# Note: using BlockSolve is incompatible with building PETSc with dynamic
# libraries on Sun4, this is because the shared library version of BlockSolve
# (which is required when BlockSolve is linked with dynamic libraries)
# does not build correctly because BlockSolve uses file names that get
# truncated with ar.
#
#BLOCKSOLVE_INCLUDE = -I/home/petsc/BlockSolve95/include
#BLOCKSOLVE_LIB     = -L/home/petsc/BlockSolve95/lib/libO/${PETSC_ARCH} -lBS95
#PETSC_HAVE_BLOCKSOLVE = -DPETSC_HAVE_BLOCKSOLVE
#
# Matlab location
#
CMEX              = /software/sun4/com/packages/matlab-5.2/bin/mex
MCC               = gcc
MATLABCOMMAND     = matlab
PETSC_HAVE_MATLAB =  -DPETSC_HAVE_MATLAB
#
# Location where adiC is installed
#
#ADIC_INCLUDE    = -I${ADIC}/run-lib/include -I${ADIC}/gradient -I${ADIC}/include -Dad_GRAD_MAX=10
#ADIC_LIB        = -L${ADIC}/lib -L${ADIC}/run-lib/lib -L${ADIC}/gradient -lADIntrinsics-C -laif_grad
#ADIC_CC         = adiC -aeud gradient -i ${PETSC_DIR}/bmake/adicmastercontrol
#PETSC_HAVE_ADIC = -DPETSC_HAVE_ADIC
#
# Location of PVODE; Alan Hindmarsh's parallel ODE solver
# 
PVODE_INCLUDE    = -I/home/petsc/software/MPI_PVODE/include
PVODE_LIB        = /home/petsc/software/MPI_PVODE/lib/sun4/libpvode.a
PETSC_HAVE_PVODE = -DPETSC_HAVE_PVODE
#
# Location of ParMetis
#
PARMETIS_INCLUDE    = -I/home/petsc/software/ParMetis-2.0
PARMETIS_LIB        = -L/home/petsc/software/ParMetis-2.0/lib/sun4 -lparmetis -lmetis
PETSC_HAVE_PARMETIS = -DPETSC_HAVE_PARMETIS
#
