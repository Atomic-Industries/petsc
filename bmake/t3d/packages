# $Id: base.site,v 1.23 1999/10/04 22:24:00 balay Exp bsmith $ 

#
#  This file contains site-specific information.  The definitions below
#  should be changed to match the locations of libraries at your site.
#  The following naming convention is used:
#     XXX_LIB - location of library XXX
#     XXX_INCLUDE - directory for include files needed for library XXX
#
# Location of BLAS and LAPACK.  These are in the Cray library "sci"
# you may have to have "-lsci" or "/mpp/lib/libsci.a" before the FC_LIB below
# but probably can just leave them blank.
#
BLAS_LIB       = 
LAPACK_LIB     = 

# Location of MPI (Message Passing Interface) software
#
#  If you are using the MPICH implementation of MPI with version number
#  less then 1.1, you should remove the -DPETSC_HAVE_INT_MPI_COMM below.
#
#  We recommend using the Cray MPI, rather than MPICH.
#
MPI_LIB        = /mpp/lib/libmpi.a
MPI_INCLUDE    =  -DPETSC_HAVE_INT_MPI_COMM -I/usr/include/mpp
MPIRUN         = /usr/local/mpi/bin/mpirun
#
# the -I/usr/include above is to insure that the C++ compiler with 
# complex finds the /usr/include/complex.h not /usr/include/mpp/complex.h
# which appears to be for C NOT c++.
#
# ----------------------------------------------------------------------------------------  
#  Locations of OPTIONAL packages. Comment out those you do not have.
# ----------------------------------------------------------------------------------------  
#
# Location of X-windows software: Cray T3d DOES NOT have X11 so you should 
# leave these blank.
#
#X11_INCLUDE    = 
#X11_LIB        = 
#PETSC_HAVE_X11 = -DPETSC_HAVE_X11
#
# Location of MPE
# If using MPICH version 1.1.2 or higher use the flag -DPETSC_HAVE_MPE_INITIALIZED_LOGGING
#
#MPE_INCLUDE   = -I/usr/local/mpi/include
#MPE_LIB       = -L/usr/local/mpi/lib/sun4/ch_p4 -lmpe -lpmpi
#PETSC_HAVE_MPE = -DPETSC_HAVE_MPE
#
# Location of BlockSolve (MPI version)
#
#BLOCKSOLVE_INCLUDE = -I/u/ddscf/bsmith/BlockSolve95/include
#BLOCKSOLVE_LIB     = -L/u/ddscf/bsmith/BlockSolve95/lib/libO/${PETSC_ARCH} -lBS95
#PETSC_HAVE_BLOCKSOLVE = -DPETSC_HAVE_BLOCKSOLVE
#
# Matlab location
#
#CMEX            = 
#MCC            = 
#MATLABCOMMAND  = matlab
#PETSC_HAVE_MATLAB =  -DPETSC_HAVE_MATLAB
#
# Location where adiC is installed
#
#ADIC_INCLUDE = -I${ADIC}/run-lib/include -I${ADIC}/gradient -I${ADIC}/include -Dad_GRAD_MAX=1
#ADIC_LIB     = -L${ADIC}/lib -L${ADIC}/run-lib/lib -L${ADIC}/gradient -lADIntrinsics-C -laif_
#ADIC_CC      = adiC -aeud gradient -i ${PETSC_DIR}/bmake/adicmastercontrol
#PETSC_HAVE_ADIC = -DPETSC_HAVE_ADIC
#
# Location of PVODE; Alan Hindmarsh's parallel ODE solver
# 
#PVODE_INCLUDE = -I/home/petsc/software/MPI_PVODE/include
#PVODE_LIB     = /home/petsc/software/MPI_PVODE/lib/sun4/libpvode.a
#PETSC_HAVE_PVODE = -DPETSC_HAVE_PVODE
#
# Location of ParMetis
#
#PARMETIS_INCLUDE = -I/home/bsmith/libraries/ParMetis.v1.0
#PARMETIS_LIB     = /home/bsmith/libraries/ParMetis.v1.0/libparmetis.a 
#PETSC_HAVE_PARMETIS = -DPETSC_HAVE_PARMETIS

