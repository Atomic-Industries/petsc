# $Id: base.site,v 1.17 2000/02/02 20:07:23 bsmith Exp balay $ 

#
#  This file contains site-specific information.  The definitions below
#  should be changed to match the locations of libraries at your site.
#  The following naming convention is used:
#     XXX_LIB - location of library XXX
#     XXX_INCLUDE - directory for include files needed for library XXX
#
# Location of BLAS and LAPACK.  See ${PETSC_DIR}/docs/installation.html
# for information on retrieving them.
#
# If your machine has the file /opt/SUNWspro/SC4.0/lib/libsunperf.a you 
# can use -lsunperf or /opt/SUNWspro/SC4.0/lib/libsunperf.a below instead of
# installing BLAS and LAPACK yourself.
#
BLAS_LIB       =  /home/petsc/software/blaslapack/blas_solaris.a
LAPACK_LIB     =  /home/petsc/software/blaslapack/lapack_solaris.a
#BLAS_LIB       =  -lsunperf -lSUNWPro_lic


#
# Location of MPI (Message Passing Interface) software
#
# Note: if ch_shmem version of mpich is used, replace -lsocket by -lthread
#
#MPI_LIB        = -L/home/petsc/mpich/lib/solaris/ch_p4/ -lmpi -lnsl -lsocket -lgen
#MPI_INCLUDE    = -I/home/petsc/mpich/include 
#MPIRUN         = /home/petsc/mpich/lib/solaris/ch_p4/mpirun
MPI_HOME       = /home/petsc/software/mpich-1.1.2
MPI_BUILD_HOME = ${MPI_HOME}/build/solaris/ch_p4
MPI_LIB        = -L${MPI_BUILD_HOME}/lib -lpmpich -lmpich -lsocket -lnsl -lnsl -laio
MPI_INCLUDE    = -I${MPI_HOME}/include -I${MPI_BUILD_HOME}/include
MPIRUN         = ${MPI_BUILD_HOME}/bin/mpirun -machinefile /home/bsmith/petsc/maint/hosts.local

#
# For the Berkeley am2 device for MPICH use
#
#MPI_LIB        = -L/usr/now/mpi/mpich/lib/solaris/am2/ -lmpi -L/usr/now/lib -L/usr/now/am2/lib \
#                 -lam2 -lglunix -lens -lglobex -lposix4 -lnsl -lsocket -lgen
#MPI_INCLUDE    = -I/usr/now/mpi/mpich/include 
#MPIRUN         = glurun
#
# ----------------------------------------------------------------------------------------  
#  Locations of OPTIONAL packages. Comment out those you do not have.
# ----------------------------------------------------------------------------------------  
#
# Location of X-windows software
#
X11_INCLUDE    = -I/usr/openwin/include
X11_LIB        = -L/usr/openwin/lib -lX11
PETSC_HAVE_X11 = -DPETSC_HAVE_X11
#
# Location of MPE
# If using MPICH version 1.1.2 or higher use the flag -DPETSC_HAVE_MPE_INITIALIZED_LOGGING
#
#MPE_INCLUDE   = -I/home/petsc/mpich/mpe
#MPE_LIB       = -L/home/petsc/mpich/lib/solaris/ch_p4/ -lmpe -lpmpi
MPE_INCLUDE   = -I${MPI_HOME}/mpe
MPE_LIB       = -L${MPI_BUILD_HOME}/lib -lmpe
PETSC_HAVE_MPE = -DPETSC_HAVE_MPE
#
# Location of BlockSolve (MPI version)
#
#BLOCKSOLVE_INCLUDE            =
#BLOCKSOLVE_LIB                =
#PETSC_HAVE_BLOCKSOLVE = -DPETSC_HAVE_BLOCKSOLVE
#
# Matlab location
#
#CMEX              = 
#MCC               = 
#MATLABCOMMAND     = matlab
#PETSC_HAVE_MATLAB =  -DPETSC_HAVE_MATLAB
#
# Location where adiC is installed
#
#ADIC_INCLUDE    = -I${ADIC}/run-lib/include -I${ADIC}/gradient -I${ADIC}/include -Dad_GRAD_MAX=1
#ADIC_LIB        = -L${ADIC}/lib -L${ADIC}/run-lib/lib -L${ADIC}/gradient -lADIntrinsics-C -laif_
#ADIC_CC         = adiC -aeud gradient -i ${PETSC_DIR}/bmake/adicmastercontrol
#PETSC_HAVE_ADIC = -DPETSC_HAVE_ADIC
#
# Location of PVODE; Alan Hindmarsh's parallel ODE solver
# 
#PVODE_INCLUDE    = 
#PVODE_LIB        = 
#PETSC_HAVE_PVODE = -DPETSC_HAVE_PVODE
#
# Location of ParMetis
#
#PARMETIS_INCLUDE    = 
#PARMETIS_LIB        = 
#PETSC_HAVE_PARMETIS = -DPETSC_HAVE_PARMETIS
#
#  Location for ALICE Memory Snooper
#
#AMS_INCLUDE    = 
#AMS_LIB        = 
#PETSC_HAVE_AMS = -DPETSC_HAVE_AMS
# 
# ---------------------------------------------------------------------------------------
#
# If you are using shared version of any external libraries you must make this
# point to the directories where all your shared libraries are stored.
#
#C_DYLIBPATH     =
#F_DYLIBPATH     =

