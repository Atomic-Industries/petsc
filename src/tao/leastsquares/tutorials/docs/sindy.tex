% \documentclass[letterpaper,12pt]{article}
\documentclass{article}
\usepackage[margin=0.9in]{geometry}

\usepackage{cite}
\usepackage{listings}
\usepackage{subcaption}
\usepackage{color}
\usepackage{amsmath,amssymb}

\usepackage{graphicx}
\usepackage{float}
\usepackage{gensymb}
\usepackage{physics}
\usepackage{listings}

\usepackage[final]{hyperref} % adds hyper links inside the generated pdf file
\hypersetup{
    colorlinks=true,       % false: boxed links; true: colored links
    linkcolor=blue,          % color of internal links
    citecolor=blue,        % color of links to bibliography
    filecolor=magenta,      % color of file links
    urlcolor=blue         
}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\lstset{frame=tb,
  language=C,
  aboveskip=1mm,
  belowskip=1mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}


\let\vec\mathbf
\def\real{\mathbb{R}}

\pdfstringdefDisableCommands{%
  \def\lstinline#1{}%
}

\begin{document}

\tableofcontents

\pagebreak
\section{Theory}

This discussion is based on \cite{brunton2016sindy} and \cite{shea2020sindy-bvp}.
I'm going to try to outline the general case. 
\begin{align}
\vec{u} &= \vec{u}(\vec{x}, t) \\
\frac{d\vec{u}}{dt} &= \frac{d\vec{u}}{dt}(\vec{u}, \vec{x}, t)
\end{align}
for $\vec{u} \in \real^{d_u}, \,\, \vec{x} \in \real^{d_x}, \,\, t \in \real$.
The goal of SINDy is to represent $\frac{d\vec{u}}{dt}$ in a basis $B$ of size
$p$ with sparse coefficients $\vec{c} \in \real^p$.
\begin{equation}
\frac{d\vec{u}}{dt} \approx \sum_{i=1}^p c_i \vec{b_i}
\end{equation}
\begin{align}
b_i \in B = \{\vec{1}, \vec{u}, \vec{x}, t, \vec{u}^2, \vec{u}\vec{x},\vec{u}t,\vec{x}^2,\vec{x}t,t^2,\hdots,\sin\vec{u},\cos{\vec{u}}, \frac{\partial\vec{u}}{\partial \vec{x}},\hdots\}
\end{align}
This will generally be represented as a linear system with discretized $\vec{U}$
and $\vec{X}$, with the columns of the matrix $\Theta$ represented basis
functions evaluated at specific $\vec{X}$ and $t$.
\begin{equation}\label{eq:basis-system}
\frac{d\vec{U}}{dt} \approx \Theta(\vec{U}, \vec{X}, t) \vec{c}
\end{equation}

Note that when $d_x \ne 1$ or $d_u \ne 1$, the vector terms in the basis shown above
aren't mathematically well-defined, but the idea is to include polynomial terms,
and these can also include polynomial terms of separate components of the
vector, and cross terms of those components. The functions shown in the basis
above are just examples; the main idea is that the basis can have any kind of
functions of $\vec{u}$, $\vec{x}$, and $t$.

Also note that, as far as the algorithm goes, it does not matter what is on the
left-hand side. The algorithm does not take advantage of it being a time
derivative of something. It could be a time derivative (\cite{brunton2016sindy})
or a spatial derivative (\cite{shea2020sindy-bvp}), or it could just be any
variable that you want to have a sparse representation for (citation needed).
Similarly, since the basis functions can be anything, the basis matrix $\Theta$
could be a function of any variables.

\subsection{Simplest dimensional case, $d_x = 0$ and $d_u = 1$} \label{section:simplest}

To get to a multi-dimensional formulation, I'll start out by describing the case when $d_x = 0$ and $d_u = 1$.
\begin{align*}
u &= u(t) \\
\frac{du}{dt} &= \frac{du}{dt}(u, t)
\end{align*}
In this case, the collected data will be a one-dimensional set of values of $u$
at different times. Let $m$ be the number of data points, and $\vec{U} \in
\real^m$ be the set of data points, where $U_k$ is the $k$-th measurement of
$u$, with a corresponding derivative $\frac{d\vec{U}}{dt}$. Let $\vec{B_i} \in
\real^m$ be the $i$-th basis function evaluated at all $m$ data points, such
that $B_{k,i}$ is the basis function evaluated at the $k$-th measurement.
Then the equation to solve is the following:
\begin{equation*}
\frac{d\vec{U}}{dt} \approx \sum_{i=1}^p c_i \vec{B_i}
\end{equation*}
This can be written as a matrix multiplication, where $\vec{B_i}$ are the
columns of $\Theta \in \real^{m \times p}$.
\begin{equation}
\frac{d\vec{U}}{dt} \approx \Theta \vec{c}
\end{equation}
This can be solved for a sparse $\vec{c}$ with a sparse least squares solver. In
this case the basis functions are functions of the two variables $u$ and $t$.

\subsection{More outputs, $d_x = 0$ and $d_u \ge 1$} \label{section:more-outputs}

When $\vec{u}$ is a vector, each measurement will have $d_u$ values. In this
case the basis functions can be made of functions of the $d_u+1$ variables
$u_1,u_2,\hdots,u_{d_u}$ and $t$. To obtain separate sparse coefficients for
each output $\frac{du_i}{dt}$, the sparse regression can be performed on each
output dimension separately. Let $\vec{U_i} \in \real^{m}$ be the measurements
for output $i$ and $\vec{c_i} \in \real^p$ be the coefficients for output $i$.
Then we can solve the equation below to find all the coefficients.
\begin{equation}\label{eq:same-theta-eqs}
\frac{d\vec{U_i}}{dt} \approx \Theta \vec{c_i} \,\,\ \text{ for $i \in \{1,2,\hdots,d_u\} $}
\end{equation}

This can be structured as a single matrix equation by letting $\vec{U_i}$ be the
columns of the matrix $\vec{U} \in \real^{m \times d_u}$, and $\vec{c_i}$ be the
columns of the matrix $\vec{c} \in \real^{p \times d_u}$. Then the equation
becomes the following:
\begin{equation*}
\frac{d\vec{U}}{dt} \approx \Theta \vec{c}
\end{equation*}
This formulation makes each output $u_i$ depend on the same set of basis
functions, stored in $\Theta$. To write it as a single matrix equation but allow
each $u_i$ to have a different set of basis functions, a block diagonal
structure can be used.

Let $\Theta^{(i)} \in \real^{m \times p_i}$ be the data for the $p_i$ basis
functions to use for output $u_i$. Let $p = \sum p_i$ be the total number of
basis functions. Then the block diagonal matrix $\Theta' \in \real^{md_u \times p}$ can be constructed
using $\Theta^{(i)}$ along the diagonal. Then flatten the data $\vec{U}$ into a
vector $\vec{U'} \in \real^{md_u}$ and flatten the coefficients $\vec{c}$ into a
vector $\vec{c}' \in \real^{p}$.
\begin{equation}\label{eq:dx0-dudt-separate-theta}
\frac{d\vec{U'}}{dt} = 
\begin{bmatrix}
\frac{d\vec{U_1}}{dt} \\ \frac{d\vec{U_2}}{dt} \\ \vdots \\ \frac{d\vec{U_{d_u}}}{dt}
\end{bmatrix}
\approx
\begin{bmatrix}
\Theta^{(1)} \\
& \Theta^{(2)} \\
& & \ddots \\
& & & \Theta^{(d_u)} \\
\end{bmatrix}
\begin{bmatrix}
\vec{c_1} \\ \vec{c_2} \\ \vdots \\ \vec{c_{d_u}}
\end{bmatrix}
= \Theta' \vec{c}'
\end{equation}
A possible problem with this formulation is that a sparse solver may enforce
sparsity on $\vec{c}$, but the desired sparsity is actually on each separate
$\vec{c_i}$. The problem is that a larger value in $\vec{c_i}$ may make the
values in $\vec{c_j}$ smaller for some $i$ and $j$. For this reason, it may be
more beneficial to use a sparse solver on the following separate equations instead:
\begin{equation}\label{eq:diff-theta-eqs}
\frac{d\vec{U_i}}{dt} \approx \Theta^{(i)} \vec{c_i} \,\,\ \text{ for $i \in \{1,2,\hdots,d_u\} $}
\end{equation}

\subsection{Handling coordinates, $d_x = 1$ and $d_u = 1$}
In this case, we have a scalar function with 1D coordinates $u = u(x,t)$. In
this case, our measurements will be snapshots of $u(x)$ at different times. Let
$n$ be the number of slices of the $x$ dimension, and $m$ be the number of
slices in the $t$ dimension. Let $\vec{U} \in \real^{m \times n}$ be the set of
data points where each column $\vec{U_j} \in \real^m = u(x_j, t)$ is the value
of $u$ at $x_j$ fixed time for each different time $t$.

This can be thought of as monitoring $n$ separate outputs similary to the case
in Section~\ref{section:more-outputs}, but additional structure can be obtained by
assuming that $u$ is governed by a PDE. In that case, $\frac{du}{dt}(u,x_j,t)$
will only depend on $x_j$, $u$, and the spatial derivatives of $u$ at $x_j$.
Thus, the linear system should be set up so that there is no dependence between
separate $x$ coordinates; each $x_j$ will have a separate basis matrix
$\Theta^{(j)}(x_j, u(x_j,t)) \in \real^{m \times p}$.

\paragraph{Varying coefficients}
This can be done in a similar manner as
Equation~\ref{eq:dx0-dudt-separate-theta}. Note that each point should use the
same set of basis functions. Let the number of basis functions at each point be
$p$. The block diagonal matrix $\Theta' \in \real^{mn \times np}$ has
$\Theta^{(j)}$ along the diagonal. Each point $x_j$ has a coefficient vector
$\vec{c_j} \in \real^p$
\begin{equation}\label{eq:coord-varying}
\frac{d\vec{U'}}{dt} = 
\begin{bmatrix}
\frac{d\vec{U_1}}{dt} \\ \frac{d\vec{U_2}}{dt} \\ \vdots \\ \frac{d\vec{U_n}}{dt}
\end{bmatrix}
\approx
\begin{bmatrix}
\Theta^{(1)} \\
& \Theta^{(2)} \\
& & \ddots \\
& & & \Theta^{(n)} \\
\end{bmatrix}
\begin{bmatrix}
\vec{c_1} \\ \vec{c_2} \\ \vdots \\ \vec{c_{n}}
\end{bmatrix}
= \Theta' \vec{c}'
\end{equation}
This formulation allows the coefficient vector to vary across space, since each
point has a separate set of basis coefficients. This formulation is used on
\cite{shea2020sindy-bvp}. An algorithm like Sequential Grouped Threshold Ridge
Regression in \cite{shea2020sindy-bvp} can be used to ensure that each $x$
coordinate uses the same sparsity for the basis functions.

\paragraph{Constant coefficients}
Alternatively, it may be desirable for the coefficients to not vary across
space; instead, a single coefficient vector should represent all points. In this
case, the input data and the matrices can be stacked so that each $x$ position
is effectively an independent data point. The matrix $\Theta' \in \real^{mn
\times p}$ can be constructed by stacking $\Theta^{(i)}$ rowwise. The
data $\vec{U}$ can be stacked into vector $\vec{U'} \in \real^{md_u}$. Each
point $x_j$ has the same coefficient vector $\vec{c} \in \real^p$

\begin{equation}\label{eq:coord-const}
\frac{d\vec{U'}}{dt} = 
\begin{bmatrix}
\frac{d\vec{U_1}}{dt} \\ \frac{d\vec{U_2}}{dt} \\ \vdots \\ \frac{d\vec{U_{n}}}{dt}
\end{bmatrix}
\approx
\begin{bmatrix}
\Theta^{(1)} \\
\Theta^{(2)} \\
\vdots \\
\Theta^{(n)} \\
\end{bmatrix}
\vec{c}
= \Theta' \vec{c}
\end{equation}

\subsection{Full multi-dimensional case, $d_x \ge 1$ and $d_u \ge 1$}
This simply combines the descriptions of the previous two sections. The $d_x$
coordinates can be linearized in some manner so that they can be referenced with
a single index $j$. Then Equation~\ref{eq:coord-varying} or~\ref{eq:coord-const}
can be used with $n$ equal to the total number of coordinate points.

Each dimension $i \in \{1,2,\hdots,d_u \}$ can be treated separately, either as
a separate column as used in Equation~\ref{eq:same-theta-eqs}
and~\ref{eq:diff-theta-eqs} or as a stack next to a block diagonal matrix as
used in Equation~\ref{eq:dx0-dudt-separate-theta}. This yields $d_u$ linear
systems so that each output dimension has its own independent set of sparse
coefficients as a solution.


\subsection{Normalization}

Let $D \in \real^{p \times p}$ be a diagonal matrix where each diagonal is the magnitude of the
corresponding column in $\Theta$. This matrix can be used to scale $\Theta$ to
get a matrix $\Theta'$ where each column has norm 1.
\begin{equation}\label{eq:normalized-system}
\frac{d\vec{U}}{dt} \approx (\Theta D^{-1}) (D\vec{c}) = \Theta' \vec{c'}
\end{equation}
The solution of the normalized system is $\vec{c'}$, and $\vec{c}$ can be
recovered using $\vec{c} = D^{-1} \vec{c'}$.

\pagebreak
\section{Examples}

\subsection{sindy\_sine}
In this example, $d_x = 0$ (no coordinates) and $d_u = 1$. There is no time dependence.
\begin{equation*}
\frac{dU}{dt} = -\sin(U) \approx -U + 0.1\overline{6}U^3 - 0.008\overline{3}U^5
\end{equation*}
The code runs the ODE from points spaced evenly around 0, with the goal of
recovering the Maclaurin series. Note that since this problem has no
coordinates, the name $x$ is used in the code instead of $U$.
\begin{lstlisting}[language={}]
                    dx/dt
 1                       0
 x             -9.9993e-01
 x*x                     0
 x*x*x          1.6626e-01
 x*x*x*x                 0
 x*x*x*x*x     -7.7962e-03
\end{lstlisting}
These values are all slightly smaller than desired. Slightly better results can
be obtained by not using regularization when doing the least squares. This can
be done with the option \lstinline{-tao_brgn_regularizer_weight 0}.
\begin{lstlisting}[language={}]
                    dx/dt
 1                       0
 x             -9.9997e-01
 x*x                     0
 x*x*x          1.6643e-01
 x*x*x*x                 0
 x*x*x*x*x     -7.9052e-03
\end{lstlisting}


\subsection{sindy\_sine\_cosine}
In this example, $d_x = 0$ (no coordinates) and $d_u = 2$. There is no time dependence.
\begin{equation*}
\frac{d\vec{U}}{dt} =
\begin{bmatrix}
-\sin(U_1) \\ \cos(U_2)
\end{bmatrix}
\approx
\begin{bmatrix}
 -U + 0.1\overline{6}U^3 - 0.008\overline{3}U^5 \\  1 - 0.5U^2 +  0.041\overline{6}U^4
\end{bmatrix}
\end{equation*}
This example effectively runs an independent ODE for each component of the vector.
The code runs the ODEs from points spaced evenly around 0, with the goal of
recovering the Maclaurin series for each term.

Note that since $\cos(x)$ is positive around 0, the ODE will evolve those values
such that they won't be centered around 0, which means the result won't be
expected to match the Maclaurin series exactly.
\begin{lstlisting}[language={}]
                                dx/dt[0]      dx/dt[1]
 1                                      0    9.9928e-01
 x[i]                         -9.9997e-01             0
 x[i]*x[i]                              0   -4.9523e-01
 x[i]*x[i]*x[i]                1.6642e-01             0
 x[i]*x[i]*x[i]*x[i]                    0    3.6669e-02
 x[i]*x[i]*x[i]*x[i]*x[i]     -7.9029e-03             0
\end{lstlisting}

In the above run, the cross term range was set to 0, so that each output
component is a function only of the corresponding input component. Without
making that constraint, noise can lead to entangled components, especially for
the non-centered cosine term. In the above runs, the data for the time
derivative was computed exactly, but in this run, a fourth-order centered finite
difference scheme is used to compute the data used for SINDy. (This was done
with the options \lstinline{-sindy_cross_term_range -1 -fd_der 1}).
\begin{lstlisting}[language={}]
                                dx/dt[0]      dx/dt[1]
 1                                      0    9.8486e-01
 x[0]                         -9.9998e-01    8.9189e-03
 x[1]                                   0    5.5136e-03
 x[0]*x[0]                              0    1.1331e-01
 x[0]*x[1]                              0   -3.0466e-02
 x[1]*x[1]                              0   -4.5908e-01
 x[0]*x[0]*x[0]                1.6643e-01   -1.3214e-01
 x[0]*x[0]*x[1]                         0   -1.6491e-02
 x[0]*x[1]*x[1]                         0   -9.6066e-03
 x[1]*x[1]*x[1]                         0   -1.6298e-02
 x[0]*x[0]*x[0]*x[0]                    0   -2.8238e-02
 x[0]*x[0]*x[0]*x[1]                    0    9.3109e-02
 x[0]*x[0]*x[1]*x[1]                    0   -1.1547e-01
 x[0]*x[1]*x[1]*x[1]                    0    2.8111e-02
 x[1]*x[1]*x[1]*x[1]                    0    1.6894e-02
 x[0]*x[0]*x[0]*x[0]*x[0]     -7.9224e-03    5.2672e-02
 x[0]*x[0]*x[0]*x[0]*x[1]               0   -4.3969e-02
 x[0]*x[0]*x[0]*x[1]*x[1]               0    3.5491e-02
 x[0]*x[0]*x[1]*x[1]*x[1]               0    5.1007e-02
 x[0]*x[1]*x[1]*x[1]*x[1]               0   -7.6728e-03
 x[1]*x[1]*x[1]*x[1]*x[1]               0    1.0493e-02
\end{lstlisting}

\subsection{sindy\_sine\_cosine\_grid}
In this example, $d_x = 2$ (2D coordinates) and $d_u = 2$. There is no time dependence.
\begin{equation*}
\frac{d\vec{U}}{dt}(x_1, x_2) =
\begin{bmatrix}
-\sin U_1(x_1,x_2) \\ \cos U_2(x_1,x_2)
\end{bmatrix}
\end{equation*}
This example effectively runs an independent ODE at each coordinate and each
component of the vector. The code runs the ODEs from points spaced evenly around
0, with the goal of recovering the Maclaurin series for each term, with the
caveat that the cosine component won't stay centered about 0.

This code essentially gives the same results as the examples above, with
enhanced problem of the regularization messing up the data. With the default
regularization parameter $10^{-4}$, the fifth order sine term is lost. Turning
it down to \lstinline{-tao_brgn_regularizer_weight 1e-7} recovers the expected
value.

An additional problem encountered here is that the the constant term in the
cosine data is lost if the left-hand side is approximated with a fourth-order
central finite difference (\lstinline{-fd_der 1}). This happens because the
finite difference doesn't approximate the first two steps or last two steps of
any run, so it zeros them out instead. This effect can be overcome by running
for more time steps so that the boundary steps have a smaller relative effect.


\subsection{lorenz}
In this example, $d_x = 0$ (no coordinates) and $d_u = 3$. There is no time dependence.
\begin{align*}
\frac{d\vec{U}}{dt} &=
\begin{bmatrix}
\sigma(U_2-U_1) \\
U_1(\rho-U_3)-U_2 \\
U_1U_2-\beta U_3;
\end{bmatrix} \\
\sigma &= 10,\,\, \beta = \frac{8}{3},\,\, \rho = 28
\end{align*}
This example runs in a straightforward manner and recovers the correct values
essentially exactly.
\begin{lstlisting}[language={}]
                      dx/dt[0]      dx/dt[1]      dx/dt[2]
 1                            0             0             0
 x[0]               -1.0000e+01    2.8000e+01             0
 x[1]                1.0000e+01   -1.0000e+00             0
 x[2]                         0             0   -2.6667e+00
 x[0]*x[0]                    0             0             0
 x[0]*x[1]                    0             0    1.0000e+00
 x[0]*x[2]                    0   -1.0000e+00             0
 x[1]*x[1]                    0             0             0
 x[1]*x[2]                    0             0             0
 x[2]*x[2]                    0             0             0
 x[0]*x[0]*x[0]               0             0             0
 x[0]*x[0]*x[1]               0             0             0
 x[0]*x[0]*x[2]               0             0             0
 x[0]*x[1]*x[1]               0             0             0
 x[0]*x[1]*x[2]               0             0             0
 x[0]*x[2]*x[2]               0             0             0
 x[1]*x[1]*x[1]               0             0             0
 x[1]*x[1]*x[2]               0             0             0
 x[1]*x[2]*x[2]               0             0             0
 x[2]*x[2]*x[2]               0             0             0
\end{lstlisting}


\subsection{lorenz96}
In this example, $d_x = 0$ (no coordinates) and $d_u = 36$. There is no time dependence.
\begin{align*}
\frac{dU_i}{dt} &= (U_{i+1} - U_{i-2}) U_{i-1} - U_i + F \\
F &= 8
\end{align*}
This system is very large. By default, SINDy would construct a basis that
include terms entangling each $U_i$. To make the system tractable, I set the
cross-term range to 2, so that only $U_{i-2},U_{i-1},U_i,U_{i+1},U_{i+2}$ would
be used in the basis function computation for output $\frac{dU_i}{dt}$. Below is
the output for the first three components; the rest are similar.
\begin{lstlisting}[language={}]
                     dx/dt[0]      dx/dt[1]      dx/dt[2]      dx/dt[3] 
 1                  7.8484e+00    7.8996e+00    7.8879e+00    7.8517e+00
 x[i-2]                      0             0             0             0
 x[i-1]                      0             0             0             0
 x[i]              -9.8401e-01   -9.9015e-01   -9.8526e-01   -9.8215e-01
 x[i+1]                      0             0             0             0
 x[i+2]                      0             0             0             0
 x[i-2]*x[i-2]               0             0             0             0
 x[i-2]*x[i-1]     -9.9822e-01   -9.9845e-01   -9.9859e-01   -9.9754e-01
 x[i-2]*x[i]                 0             0             0             0
 x[i-2]*x[i+1]               0             0             0             0
 x[i-2]*x[i+2]               0             0             0             0
 x[i-1]*x[i-1]               0             0             0             0
 x[i-1]*x[i]                 0             0             0             0
 x[i-1]*x[i+1]      1.0002e+00    9.9959e-01    9.9914e-01    9.9933e-01
 x[i-1]*x[i+2]               0             0             0             0
 x[i]*x[i]                   0             0             0             0
 x[i]*x[i+1]                 0             0             0             0
 x[i]*x[i+2]                 0             0             0             0
 x[i+1]*x[i+1]               0             0             0             0
 x[i+1]*x[i+2]               0             0             0             0
 x[i+2]*x[i+2]               0             0             0             0
\end{lstlisting}

\subsection{pde\_power\_grid}

In this example, $d_x = 2$ and $d_u = 1$, and there is time dependence.
\begin{align*}
\frac{dU}{dt} &= - \frac{\partial x_1}{\partial t} \frac{\partial U}{\partial x_1}
                 - \frac{\partial x_2}{\partial t} \frac{\partial U}{\partial x_2}
                 + f(t) \frac{\partial^2 U}{\partial x_2^2}
\\ \frac{\partial x_1}{\partial t} &= (x_2 - \omega_s)
\\ \frac{\partial x_2}{\partial t} &= \frac{\omega_s}{2H}(P_m - P_{max}\sin(x_1))
\\ f(t) &= \left(\frac{\lambda \omega_s}{2H}\right) ^ 2 q (1-e^{-t/\lambda})
\end{align*}
\begin{lstlisting}[language={}]
                                                    du/dt
 1                                                       0
 du/dx                                          1.0000e+00
 du/dy                                         -1.0000e-01
 d2u/dy2                                                 0
 d2u/dx2                                                 0
 t                                                       0
 (1 - exp(-t/lambda))                                    0
 x[0]                                                    0
 x[1]                                                    0
 du/dx*du/dx                                             0
 du/dx*du/dy                                             0
 du/dx*d2u/dy2                                           0
 du/dx*d2u/dx2                                           0
 du/dx*t                                                 0
 du/dx*(1 - exp(-t/lambda))                              0
 du/dx*x[0]                                              0
 du/dx*x[1]                                    -1.0000e+00
 du/dy*du/dy                                             0
 du/dy*d2u/dy2                                           0
 du/dy*d2u/dx2                                           0
 du/dy*t                                                 0
 du/dy*(1 - exp(-t/lambda))                              0
 du/dy*x[0]                                     2.1000e-01
 du/dy*x[1]                                              0
 d2u/dy2*d2u/dy2                                         0
 d2u/dy2*d2u/dx2                                         0
 d2u/dy2*t                                               0
 d2u/dy2*(1 - exp(-t/lambda))                   9.9999e-05
 d2u/dy2*x[0]                                            0
 d2u/dy2*x[1]                                            0
 d2u/dx2*d2u/dx2                                         0
 d2u/dx2*t                                               0
 d2u/dx2*(1 - exp(-t/lambda))                            0
 d2u/dx2*x[0]                                            0
 d2u/dx2*x[1]                                            0
 t*t                                                     0
 t*(1 - exp(-t/lambda))                                  0
 t*x[0]                                                  0
 t*x[1]                                                  0
 (1 - exp(-t/lambda))*(1 - exp(-t/lambda))               0
 (1 - exp(-t/lambda))*x[0]                               0
 (1 - exp(-t/lambda))*x[1]                               0
 x[0]*x[0]                                               0
 x[0]*x[1]                                               0
 x[1]*x[1]                                               0
\end{lstlisting}

\pagebreak
\section{Implementation overview}

The basis system is below (copy/pasted from Equation~\ref{eq:basis-system}).
\begin{equation}
\frac{d\vec{U}}{dt} \approx \Theta(\vec{U}, \vec{X}, t) \vec{c}
\end{equation}
An implementation requires the following steps:
\begin{enumerate}
    \item Compute the input data $\vec{U}$ (at coordinates $\vec{X}$ and times $t$). This could be data read from a file or generated with a timestepper.
    \item Compute the left-hand side $\frac{d\vec{U}}{dt}$. This can be computed with finite differences or similar methods.
    \item Compute the basis $\Theta$, which requires choosing basis functions.
    \item Compute the coefficients $\vec{c}$. This can be done with any sparse regression algorithm.
    \item Use $\vec{c}$ and the basis functions to generate $\frac{d\vec{U}}{dt}$ at the same data points or at new data points. (I have not done this yet).
\end{enumerate}

\subsection{Data generation}

I'm using PETSc's \lstinline{TS} for Step 1. I assume a fixed-time step size and
allocate memory for two arrays of \lstinline{Vec}s to hold $\vec{U}$ and
$\frac{d\vec{U}}{dt}$. Then in the TS's \lstinline{PostStep}, I record the
current value of $\vec{U}$. I have an option (in some examples) to either record
the exact value of the derivative $\frac{d\vec{U}}{dt}$ in the PostStep, or a
fourth-order central difference approximation after the run is complete.


\subsection{Basis computation}

In order for the code to work with different $d_u$ and $d_x$, the computation of
the basis needs to be flexible in how it reads the input data. The data
for a particle moving around might be stored in a 1D \lstinline{Vec} or a
\lstinline{PetscScalar} array, while the data for a PDE may be stored in a
\lstinline{Vec} with a \lstinline{DMDA}. To abstract away the data access
details, I set up a \lstinline{Variable} class. An instance of the class tracks
its data and knows how to access it properly. It also tracks its name for nice
printouts.

From the user perspective, computing the basis simply requires defining the variables
that will be present in the basis.

When the user passes these variables off to the backend, they are used to
generate the basis matrix $\Theta$.
I currently have the basis matrix set up as $d_u$ matrices as is shown in Equation~\ref{eq:diff-theta-eqs}, where
each of those matrices is constant across space, as is done in Equation~\ref{eq:coord-const}.


\subsection{Coefficient computation}

The previous step computes $\Theta$, so all this step needs to do is run a
sparse regression and return the results. I currently have Sequential
Thresholded Least Squares implemented, with parameters specified in a
\lstinline{SparseReg} class. More backends could be tested here, and we could
set sparse regression algorithm as a command-line argument.

\pagebreak
\section{Implementation specifics}

\subsection{Cross terms}
I ran into an issue with the Lorenz 96 system in that it required polynomial
terms of degree 2, but it contains 36 independent variables. I used
Equation~\ref{eq:same-theta-eqs} to set up the system, meaning each variable
uses the same basis matrix.

The basis function generator would originally create basis functions that had
all the cross terms. In this case, that would lead to $\binom{36 + 2}{2} = 703$
basis functions. Then the algorithm takes forever to run. In this case, we know
that only nearby terms will affect each other, so I added a
\lstinline{cross_term_range} parameter, which I'll call $c$. For output DOF $i$,
it will only include input DOFs $i-c,i-c+1,\hdots,i,\hdots,i+c$. So there will
be a total of $d = 2*c+1$ terms included. For $c=2$ reduces the number of basis
functions from 703 to $\binom{d+2}{2} = \binom{7}{2} = 21$, which is much more
manageable. Since each output DOF has a different set up of basis functions,
this required changing from Equation~\ref{eq:same-theta-eqs} to
Equation~\ref{eq:diff-theta-eqs} to maintain a separate basis matrix for each
output DOF.

This setup also helps the sine-cosine example. By setting the cross term range
to 0, there is no intermingling of $U_1$ and $U_2$ in the basis functions.

For examples with coordinates ($d_x \ne 0$), the cross term range currently only
applies to DOFs within a point. I.e., for a point's output DOF $i$, it will only
include input DOFs $i-c,i-c+1,\hdots,i,\hdots,i+c$ at that point.

\subsection{Basis matrix generation}

\lstinline{SINDyBasisAddVariables} sets up the basis matrices. There is a
separate matrix for each output DOF, so there are $d_u$ matrices to build.
Each matrix is built by looping through each basis function, and for each basis function,
looping through each output coordinate and adding all $m$ outputs to the for that coordinate.
This iteration procedure generates $\Theta'$ from Equation~\ref{eq:coord-const} column-wise.


\section{Things to do}

\begin{itemize}

    \item Make it possible to use the computed coefficients to get the
      $\frac{d\vec{U}}{dt}$ approximation at a specific point or sets of points.
      A good way to do this may be to restructure the basis to make it easy to
      apply to a dataset and get back the computated basis values as some data
      structure that can then be multiplied with the computed coefficients.

    \item Handle bad data points better. Perhaps supporting an interface to
      specify which points to not include in the linear system. (Current bad
      effects: the system has erroneous data that will mess up the regression.
      In the first case below, this will simply make the constant term more
      likely to be 0, and in the second case below, it may mess up all the
      results.)
    \begin{itemize}
        \item For example, when doing a fourth-order centered finite difference
          to estimate $\frac{du}{dt}$, the first two points and last two points
          don't have an estimate. I have been setting them to 0, but it would be
          good to support ignoring specific input data points.
        \item For example, on Dirichlet boundaries, $\frac{du}{dt} = 0$ no
          matter what the PDE equations are, so those points should be thrown
          out. This is different from the above problem because the above
          problem throws out a whole \lstinline{Vec}, while this requires
          throwing out specific values within a \lstinline{Vec}.
    \end{itemize}

    \item Handle threshold selection better.
    \begin{itemize}
        \item It would be good to be able to specify different thresholds for
          different degrees of freedom.
        \item It would be good to be to have automated methods for picking a
          threshold. E.g., something like Section 3.3 in \cite{shea2020sindy-bvp}.
    \end{itemize}

    \item Do non-regularized least squares after thresholding is complete? This
      will make the selected non-zero coefficients better match the data.

    \item Is there a way to do normalization better? One problem is that the
      current column scaling means coefficients get bigger and bigger as more
      data is added, which means the threshold parameter and regularization
      factor need to be changed accordingly. Maybe a row-scaling would be good.

    \item Add ADMM as an option to do the regularized least squares? I think the
      sparse regression step in SINDy should call a backend that can be swapped
      out for various least squares algorithms.

    \item Add support to write a computed basis to a file and read it from a
      file. This will let different sparse regularization strategies be tested
      without regenerating the data.

\end{itemize}


\bibliographystyle{unsrt}
\bibliography{sources}


\end{document}