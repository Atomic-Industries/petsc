% \documentclass[letterpaper,12pt]{article}
\documentclass{article}
\usepackage[margin=1in]{geometry}

\usepackage{cite}
\usepackage{listings}
\usepackage{subcaption}
\usepackage{color}
\usepackage{amsmath,amssymb}

\usepackage{graphicx}
\usepackage{float}
\usepackage{gensymb}
\usepackage{physics}
\usepackage{listings}

\usepackage[final]{hyperref} % adds hyper links inside the generated pdf file
\hypersetup{
    colorlinks=true,       % false: boxed links; true: colored links
    linkcolor=blue,          % color of internal links
    citecolor=blue,        % color of links to bibliography
    filecolor=magenta,      % color of file links
    urlcolor=blue         
}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}


\let\vec\mathbf
\def\real{\mathbb{R}}

\pdfstringdefDisableCommands{%
  \def\lstinline#1{}%
}

\begin{document}

\section{Theory}

This discussion is based on \cite{brunton2016sindy} and \cite{shea2020sindy-bvp}.
I'm going to try to outline the general case. 
\begin{align}
\vec{u} &= \vec{u}(\vec{x}, t) \\
\frac{d\vec{u}}{dt} &= \frac{d\vec{u}}{dt}(\vec{u}, \vec{x}, t)
\end{align}
for $\vec{u} \in \real^{d_u}, \,\, \vec{x} \in \real^{d_x}, \,\, t \in \real$.
The goal of SINDy is to represent $\frac{d\vec{u}}{dt}$ in a basis $B$ of size
$p$ with sparse coefficients $\vec{c} \in \real^p$.
\begin{equation}
\frac{d\vec{u}}{dt} \approx \sum_{i=1}^p c_i \vec{b_i}
\end{equation}
\begin{align}
b_i \in B = \{\vec{1}, \vec{u}, \vec{x}, t, \vec{u}^2, \vec{u}\vec{x},\vec{u}t,\vec{x}^2,\vec{x}t,t^2,\hdots,\sin\vec{u},\cos{\vec{u}}, \frac{\partial\vec{u}}{\partial \vec{x}},\hdots\}
\end{align}
This will generally be represented as a linear system with discretized $\vec{U}$
and $\vec{X}$, with the columns of the matrix $\Theta$ represented basis
functions evaluated at specific $\vec{X}$ and $t$.
\begin{equation}\label{eq:basis-system}
\frac{d\vec{U}}{dt} \approx \Theta(\vec{U}, \vec{X}, t) c
\end{equation}

Note that when $d_x \ne 1$ or $d_u \ne 1$, the vector terms in the basis shown above
aren't mathematically well-defined, but the idea is to include polynomial terms,
and these can also include polynomial terms of separate components of the
vector, and cross terms of those components. The functions shown in the basis
above are just examples; the main idea is that the basis can have any kind of
functions of $\vec{u}$, $\vec{x}$, and $t$.

Also note that, as far as the algorithm goes, it does not matter what is on the
left-hand side. The algorithm does not take advantage of it being a time
derivative of something. It could be a time derivative (\cite{brunton2016sindy})
or a spatial derivative (\cite{shea2020sindy-bvp}), or it could just be any
variable that you want to have a sparse representation for (citation needed).
Similarly, since the basis functions can be anything, the basis matrix $\Theta$
could be a function of any variables.

\subsection{Simplest dimensional case, $d_x = 0$ and $d_u = 1$} \label{section:simplest}

To get to a multi-dimensional formulation, I'll start out by describing the case when $d_x = 0$ and $d_u = 1$.
\begin{align*}
u &= u(t) \\
\frac{du}{dt} &= \frac{du}{dt}(u, t)
\end{align*}
In this case, the collected data will be a one-dimensional set of values of $u$
at different times. Let $m$ be the number of data points, and $\vec{U} \in
\real^m$ be the set of data points, where $U_k$ is the $k$-th measurement of
$u$, with a corresponding derivative $\frac{d\vec{U}}{dt}$. Let $\vec{B_i} \in
\real^m$ be the $i$-th basis function evaluated at all $m$ data points, such
that $B_{k,i}$ is the basis function evaluated at the $k$-th measurement.
Then the equation to solve is the following:
\begin{equation*}
\frac{d\vec{U}}{dt} \approx \sum_{i=1}^p c_i \vec{B_i}
\end{equation*}
This can be written as a matrix multiplication, where $\vec{B_i}$ are the
columns of $\Theta \in \real^{m \times p}$.
\begin{equation}
\frac{d\vec{U}}{dt} \approx \Theta \vec{c}
\end{equation}
This can be solved for a sparse $\vec{c}$ with a sparse least squares solver. In
this case the basis functions are functions of the two variables $u$ and $t$.

\subsection{More outputs, $d_x = 0$ and $d_u \ge 1$} \label{section:more-outputs}

When $\vec{u}$ is a vector, each measurement will have $d_u$ values. In this
case the basis functions can be made of functions of the $d_u+1$ variables
$u_1,u_2,\hdots,u_{d_u}$ and $t$. To obtain separate sparse coefficients for
each output $\frac{du_i}{dt}$, the sparse regression can be performed on each
output dimension separately. Let $\vec{U_i} \in \real^{m}$ be the measurements
for output $i$ and $\vec{c_i} \in \real^p$ be the coefficients for output $i$.
Then we can solve the equation below to find all the coefficients.
\begin{equation}\label{eq:same-theta-eqs}
\frac{d\vec{U_i}}{dt} \approx \Theta \vec{c_i} \,\,\ \text{ for $i \in \{1,2,\hdots,d_u\} $}
\end{equation}

This can be structured as a single matrix equation by letting $\vec{U_i}$ be the
columns of the matrix $\vec{U} \in \real^{m \times d_u}$, and $\vec{c_i}$ be the
columns of the matrix $\vec{c} \in \real^{p \times d_u}$. Then the equation
becomes the following:
\begin{equation*}
\frac{d\vec{U}}{dt} \approx \Theta \vec{c}
\end{equation*}
This formulation makes each output $u_i$ depend on the same set of basis
functions, stored in $\Theta$. To write it as a single matrix equation but allow
each $u_i$ to have a different set of basis functions, a block diagonal
structure can be used.

Let $\Theta^{(i)} \in \real^{m \times p_i}$ be the data for the $p_i$ basis
functions to use for output $u_i$. Let $p = \sum p_i$ be the total number of
basis functions. Then the block diagonal matrix $\Theta' \in \real^{md_u \times p}$ can be constructed
using $\Theta^{(i)}$ along the diagonal. Then flatten the data $\vec{U}$ into a
vector $\vec{U'} \in \real^{md_u}$ and flatten the coefficients $\vec{c}$ into a
vector $\vec{c}' \in \real^{p}$.
\begin{equation}\label{eq:dx0-dudt-separate-theta}
\frac{d\vec{U'}}{dt} = 
\begin{bmatrix}
\frac{d\vec{U_1}}{dt} \\ \frac{d\vec{U_2}}{dt} \\ \vdots \\ \frac{d\vec{U_{d_u}}}{dt}
\end{bmatrix}
\approx
\begin{bmatrix}
\Theta^{(1)} \\
& \Theta^{(2)} \\
& & \ddots \\
& & & \Theta^{(d_u)} \\
\end{bmatrix}
\begin{bmatrix}
\vec{c_1} \\ \vec{c_2} \\ \vdots \\ \vec{c_{d_u}}
\end{bmatrix}
= \Theta' \vec{c}'
\end{equation}
A possible problem with this formulation is that a sparse solver may enforce
sparsity on $\vec{c}$, but the desired sparsity is actually on each separate
$\vec{c_i}$. The problem is that a larger value in $\vec{c_i}$ may make the
values in $\vec{c_j}$ smaller for some $i$ and $j$. For this reason, it may be
more beneficial to use a sparse solver on the following separate equations instead:
\begin{equation}\label{eq:diff-theta-eqs}
\frac{d\vec{U_i}}{dt} \approx \Theta^{(i)} \vec{c_i} \,\,\ \text{ for $i \in \{1,2,\hdots,d_u\} $}
\end{equation}

\subsection{Handling coordinates, $d_x = 1$ and $d_u = 1$}
In this case, we have a scalar function with 1D coordinates $u = u(x,t)$. In
this case, our measurements will be snapshots of $u(x)$ at different times. Let
$n$ be the number of slices of the $x$ dimension, and $m$ be the number of
slices in the $t$ dimension. Let $\vec{U} \in \real^{m \times n}$ be the set of
data points where each column $\vec{U_j} \in \real^m = u(x_j, t)$ is the value
of $u$ at $x_j$ fixed time for each different time $t$.

This can be thought of as monitoring $n$ separate outputs similary to the case
in Section~\ref{section:more-outputs}, but additional structure can be obtained by
assuming that $u$ is governed by a PDE. In that case, $\frac{du}{dt}(u,x_j,t)$
will only depend on $x_j$, $u$, and the spatial derivatives of $u$ at $x_j$.
Thus, the linear system should be set up so that there is no dependence between
separate $x$ coordinates; each $x_j$ will have a separate basis matrix
$\Theta^{(j)}(x_j, u(x_j,t)) \in \real^{m \times p}$.

\paragraph{Varying coefficients}
This can be done in a similar manner as
Equation~\ref{eq:dx0-dudt-separate-theta}. Note that each point should use the
same set of basis functions. Let the number of basis functions at each point be
$p$. The block diagonal matrix $\Theta' \in \real^{mn \times np}$ has
$\Theta^{(j)}$ along the diagonal. Each point $x_j$ has a coefficient vector
$\vec{c_j} \in \real^p$
\begin{equation}\label{eq:coord-varying}
\frac{d\vec{U'}}{dt} = 
\begin{bmatrix}
\frac{d\vec{U_1}}{dt} \\ \frac{d\vec{U_2}}{dt} \\ \vdots \\ \frac{d\vec{U_n}}{dt}
\end{bmatrix}
\approx
\begin{bmatrix}
\Theta^{(1)} \\
& \Theta^{(2)} \\
& & \ddots \\
& & & \Theta^{(n)} \\
\end{bmatrix}
\begin{bmatrix}
\vec{c_1} \\ \vec{c_2} \\ \vdots \\ \vec{c_{n}}
\end{bmatrix}
= \Theta' \vec{c}'
\end{equation}
This formulation allows the coefficient vector to vary across space, since each
point has a separate set of basis coefficients. This formulation is used on
\cite{shea2020sindy-bvp}. An algorithm like Sequential Grouped Threshold Ridge
Regression in \cite{shea2020sindy-bvp} can be used to ensure that each $x$
coordinate uses the same sparsity for the basis functions.

\paragraph{Constant coefficients}
Alternatively, it may be desirable for the coefficients to not vary across
space; instead, a single coefficient vector should represent all points. In this
case, the input data and the matrices can be stacked so that each $x$ position
is effectively an independent data point. The matrix $\Theta' \in \real^{mn
\times p}$ can be constructed by stacking $\Theta^{(i)}$ rowwise. The
data $\vec{U}$ can be stacked into vector $\vec{U'} \in \real^{md_u}$. Each
point $x_j$ has the same coefficient vector $\vec{c} \in \real^p$

\begin{equation}\label{eq:coord-const}
\frac{d\vec{U'}}{dt} = 
\begin{bmatrix}
\frac{d\vec{U_1}}{dt} \\ \frac{d\vec{U_2}}{dt} \\ \vdots \\ \frac{d\vec{U_{n}}}{dt}
\end{bmatrix}
\approx
\begin{bmatrix}
\Theta^{(1)} \\
\Theta^{(2)} \\
\vdots \\
\Theta^{(n)} \\
\end{bmatrix}
\vec{c}
= \Theta' \vec{c}
\end{equation}

\subsection{Full multi-dimensional case, $d_x \ge 1$ and $d_u \ge 1$}
This simply combines the descriptions of the previous two sections. The $d_x$
coordinates can be linearized in some manner so that they can be referenced with
a single index $j$. Then Equation~\ref{eq:coord-varying} or~\ref{eq:coord-const}
can be used with $n$ equal to the total number of coordinate points.

Each dimension $i \in \{1,2,\hdots,d_u \}$ can be treated separately, either as
a separate column as used in Equation~\ref{eq:same-theta-eqs}
and~\ref{eq:diff-theta-eqs} or as a stack next to a block diagonal matrix as
used in Equation~\ref{eq:dx0-dudt-separate-theta}. This yields $d_u$ linear
systems so that each output dimension has its own independent set of sparse
coefficients as a solution.


\section{Examples}

\subsection{sindy\_sine}
In this example, $d_x = 0$ (no coordinates) and $d_u = 1$. There is no time dependence.
\begin{equation*}
\frac{dU}{dt} = -\sin(U)
\end{equation*}


\subsection{sindy\_sine\_cosine}
In this example, $d_x = 0$ (no coordinates) and $d_u = 2$. There is no time dependence.
\begin{equation*}
\frac{d\vec{U}}{dt} =
\begin{bmatrix}
-\sin(U_1) \\ \cos(U_2)
\end{bmatrix}
\end{equation*}


\subsection{sindy\_sine\_cosine\_grid}
In this example, $d_x = 2$ (2D coordinates) and $d_u = 2$. There is no time dependence.
\begin{equation*}
\frac{d\vec{U}}{dt}(x_1, x_2) =
\begin{bmatrix}
-\sin U_1(x_1,x_2) \\ \cos U_2(x_1,x_2)
\end{bmatrix}
\end{equation*}


\subsection{lorenz}
In this example, $d_x = 0$ (no coordinates) and $d_u = 3$. There is no time dependence.
\begin{align*}
\frac{d\vec{U}}{dt} &=
\begin{bmatrix}
\sigma(U_2-U_1) \\
U_1(\rho-U_3)-U_2 \\
U_1U_2-\beta U_3;
\end{bmatrix} \\
\sigma &= 10,\,\, \beta = \frac{8}{3},\,\, \rho = 28
\end{align*}


\subsection{lorenz96}
In this example, $d_x = 0$ (no coordinates) and $d_u = 36$. There is no time dependence.
\begin{align*}
\frac{dU_i}{dt} &= (U_{i+1} - U_{i-2}) U_{i-1} - U_i + F \\
F &= 8
\end{align*}

\subsection{pde\_power\_grid}
In this example, $d_x = 2$ and $d_u = 1$, and there is time dependence.
\begin{align*}
\frac{dU}{dt} &= - \frac{\partial x_1}{\partial t} \frac{\partial U}{\partial x_1}
                 - \frac{\partial x_2}{\partial t} \frac{\partial U}{\partial x_2}
                 + f(t) \frac{\partial^2 U}{\partial x_2^2}
\\ \frac{\partial x_1}{\partial t} &= (x_2 - \omega_s)
\\ \frac{\partial x_2}{\partial t} &= \frac{\omega_s}{2H}(P_m - P_{max}\sin(x_1))
\\ f(t) &= \left(\frac{\lambda \omega_s}{2H}\right) ^ 2 q (1-e^{-t/\lambda})
\end{align*}

\section{Implementation}

The basis system is below (copy/pasted from Equation~\ref{eq:basis-system}).
\begin{equation}
\frac{d\vec{U}}{dt} \approx \Theta(\vec{U}, \vec{X}, t) c
\end{equation}
An implementation requires the following steps:
\begin{enumerate}
    \item Compute the input data $\vec{U}$ (at coordinates $\vec{X}$ and times $t$). This could be data read from a file or generated with a timestepper.
    \item Compute the left-hand side $\frac{d\vec{U}}{dt}$. This can be computed with finite differences or similar methods.
    \item Compute the basis $\Theta$, which requires choosing basis functions.
    \item Compute the coefficients $\vec{c}$. This can be done with any sparse regression algorithm.
    \item Use $\vec{c}$ and the basis functions to generate $\frac{d\vec{U}}{dt}$ at the same data points or at new data points. (I have not done this yet).
\end{enumerate}

\subsection{Data generation}

I'm using PETSc's \lstinline{TS} for Step 1. I assume a fixed-time step size and
allocate memory for two arrays of \lstinline{Vec}s to hold $\vec{U}$ and
$\frac{d\vec{U}}{dt}$. Then in the TS's \lstinline{PostStep}, I record the
current value of $\vec{U}$. I have an option to either record the exact value of
the derivative $\frac{d\vec{U}}{dt}$ in the PostStep, or a fourth-order central
difference approximation after the run is complete.


\subsection{Basis computation}

In order for the code to work with different $d_u$ and $d_x$, the computation of
the basis needs to be flexible in how it reads the input data. The data
for a particle moving around might be stored in a 1D \lstinline{Vec} or a
\lstinline{PetscScalar} array, while the data for a PDE may be stored in a
\lstinline{Vec} with a \lstinline{DMDA}. To abstract away the data access
details, I set up a \lstinline{Variable} class. An instance of the class tracks
its data and knows how to access it properly. It also tracks its name for nice
printouts.

From the user perspective, computing the basis simply requires defining the variables
that will be present in the basis.

When the user passes these variables off to the backend, they are used to
generate the basis matrix $\Theta$.
I currently have the basis matrix set up as $d_u$ matrices as is shown in Equation~\ref{eq:diff-theta-eqs}, where
each of those matrices is constant across space, as is done in Equation~\ref{eq:coord-const}.


\subsection{Coefficient computation}

The previous step computes $\Theta$, so all this step needs to do is run a
sparse regression and return the results. I currently have Sequential
Thresholded Least Squares implemented, with parameters specified in a
\lstinline{SparseReg} class. More backends could be tested here, and we could
set sparse regression algorithm as a command-line argument.



\bibliographystyle{unsrt}
\bibliography{sources}




\end{document}