
Cost code for library 56696-01-145

PETSc grammar review:
 - Distinction between words "then" and "than"
    - THAN: used in comparisons
       - PETSc is faster THAN, better THAN, more flexible THAN, ...
         all other software in the world :)
    - THEN: used with "if/then" or to indicate time
         If you use PETSc, THEN all your coding worries will be solved ;)
         First preallocate matrix space; THEN assemble the matrix.

----------------------------------------------------------------

Doctext notes:
 - CANNOT put $ in the midst of a set of + ... -
 - MUST have text after hyphen when using . (or +/-) i.e.,
       . -option_name
   will screw things up majorly.  Instead, use
       . -option_name - Description of option

----------------------------------------------------------------

PETSc tutorial material:
   /home/gropp/petsc_work/byoc

Whenever you introduce ANYTHING new in PETSc, be sure to add 
it to the Fortran include files and fix the interface files as
well if necessary; otherwise, indicate that this still needs to be
fixed in the ThingsToDo file.

PETSc Users Mailing list info:
  Perl script for WWW petsc-users subscription is located in:
  charlotte:/www/ncsa_httpd/cgi-bin/petsc/subscribe-to-petsc-users
            (MUST access from charlotte) ... NOT currently used
  To add this to petsc.html, just add hyperlink to petsc-users.html

  petsc-users mailing list password is bsbglmdk
approve bsbglmdk subscribe petsc-users kamath2@llnl.gov

To approve a petsc-users mail in elm use | then approve

General WWW info:
  Steps for making the PETSc picture gif file for petsc.html:
    - draw figure in petscwww.fig and scale appropriately
    - start xv, then grab the region and save as petscwww.gif

Converting from Chameleon to MPI:
  See /home/gropp/chameleon/README.  The converter program is
  /home/gropp/tools.n/bin/inlinecomm.

PETScView info:
  The initial file for PETScView configuration is petsc/bin/.petscviewrc
  Although we're RCS-ing this file, it must remain checked-out so that
  the owner has write permission.  Otherwise, each new PETScView user
  will be given a default .petscviewrc that is not writable to him.


------------Accounts (around the world and around the block) -----


Berkeley: i86pc machines running Solaris
   
   millennium.Millennium.Berkeley.EDU (use ssh to log in)
   Userid: bfsmith
   Password: in Barry's office

LIBS        = -L/usr/now/mpi/mpich/lib/solaris/am2i86pc -L/usr/now/lib\
              -lmpi -lam2 -lens -leutl -lglunix -lthread -lposix4\
              -lsocket -lnsl

INCLUDES    = -I/usr/now/mpi/mpich/include

LLNL: unsecure

   IBM SP2: blue.llnl.gov
   DEC alpha cluster east.llnl.gov (named after points on the compass)

   Userid: bsmith
   Password: in Barry's office

   
---------

MCS:
1. mcs network 
    Solaris machines: caffeine,tiamat,dew
    DEC Alpha: ptera,dactile
    Linux    : timewalk,raist,flight
    Freebsd  : homer, snowy
2. ccsthome  (quad)
3. sphome    (bonnie/clyde)
4. Cave Lab  (alaska)
   Cave Power Challenge (yukon)
     user: petsc
     passwd: petsc-passwd
5. multimedia lab SP2
    user: balay,curfman,bsmith

---------

Multimedia lab SP2:

 - compile on: crag.mcs.anl.gov
    - doesn't have ESSL
    - doesn't have MPI (run directly with POE by rlogging into a node)
    - wide nodes are:  [eyas5,eyas11,eyas13,eyas15].mcs.anl.gov
 - to run directly via POE:
    - set environmental variables, e.g.,
        setenv MP_PROCS 4
        setenv MP_HOSTFILE "/home/curfman/mmsp2.host.list"
        setenv MP_EUILIB us
        setenv MP_EUIDEVICE css0
        setenv MP_RMPOOL 0
        setenv MP_PULSE 0
        setenv MP_INFOLEVEL 0
    - then run via:  poe ex1
      This will use MP_PROCS processors, as listed in MP_HOSTFILE
    - Note:  You are _supposed_ to be able to override these environmental
      variables with runtime options (see POE manpage for complete list):
          poe ex1 -hostfile hostfilename -procs n_processors
      where hostfile lists host nodes; there is a sample hostfile in
      /home/curfman/mmsp2.host.list
      Although this used to work for me (Lois), it's not working now.

 Info on POE:
    http://www.tc.cornell.edu/Edu/Tutor/POE
    http://www.mhpcc.edu/training/workshop/html/poe/poe.html

---------------

Nersc Cray T3E
machine: mcurie.nersc.gov
login:   bsmith,balay
passwd:  petsc-password

scheduler : qsub, which is actually a link to cqsub (mpirun also works)
help : consult@nersc.gov
1 800 666 3772 select option 3

grmview shows jobs running on pes
or tstat 
qstat -a lists queues
cqstatl - see all NQE requests
ps -efMP - process status for application PEs 

performance analysis tool (pat): use "man pat" for info
scratch space: /usr/tmp/[bsmith]  or $TMPDIR or $BIG

NERSc contact for maintaining the PETSc "module" 
Jonathan Carter <jcarter@Nersc.GOV>
Run module use petsc.2.0.22 to setup to us PETSc 
PETSc is located in  /usr/local/pkg/acts/PETSc.
Use the group/accout 'mpccc1' while installing/modifying PETSc

Debugging on the Cray t3e:  use totalview:
  With totalview you don't use mpirun; you just run totalview
  programname. Also when linking to use totalview you must 
  hardwire the number of processors, so add -Xnumberprocessors 
  to the link line in the makefile.  Or do at runtime via
    totalview -X numberprocessors programname

---------------
NCSA Convex Exemplar account
Machine: billie.ncsa.uiuc.edu
Machine: lena.ncsa.uiuc.edu (old)
userid: barrys,balay
password: petsc-password
Account (whatever that means?) ryc

NCSA Origin2000
Machine: modi4.ncsa.uiuc.edu
userid: barrys,balay
password: petsc-password
Account (whatever that means?) ryc
help: consult@ncsa.uiuc.edu, future@ncsa.uiuc.edu

NCSA Power Challenge 
Machine: sif.ncsa.uiuc.edu loki.ncsa.uiuc.edu  odin
Userid: barrys,balay
password: petsc-passwd
Account (whatever that means?) ryc

Other Machines at NCSA:
solaris: pecos
sun4   : space
hpux   : sangamon
userid : barrys
password: petsc-password

Account info at NCSA - command: usage

AFS :afs directory at NCSA /afs/ncsa/user/barrys,balay
To use afs on powerchallenge/convex exemplar, log into afs using klog
other afs commands to aremember:fs,pts
http://www.ncsa.uiuc.edu/Pubs/UserGuides/AFSGuide/AFSv2.1Book.html

UniTree - mass storage system: 
Machine: mss.ncsa.uiuc.edu
Userid: barrys,balay
password: petsc-passwd
useful commands: msscmd
Acccess to unitree is through ftp


Batch Jobs: batch jobs on SGIs ( Challange/Origin2000) can be submitted
by using bsub command. Sample scripts for batch submission are located 
on modi4.ncsa.uiuc.edu:~/batch-scripts/
Other useful commands 
bqueues: List all the queues avilable
bjobs  : list all the batch jobs started.
bjobs -u all : list all the batch jobs started by all users
bkill : kill a batch job
usage : account usage info
(and other b-commands and lsbatch commands)

---------
University of Bergen

SGI Origin 2000
machine  : ask.ii.uib.no ,dontask
userid   : bsmith
password : petsc-password
Scheduler: lsf (similar to NCSA - commands: bjobs, bqueues etc)

---------
UT account:
  use seaboard.pe.utexas.edu (also have brazos.pe.utexas.edu  and frankfort)
  login ID: bsmith
  password: petsc-password
 
brahma.ticam.utexas.edu 
  login id: bsmith,sbalay
  passwd  : petsc-password 

SP2 at TICAM
hydra01-015 or sp101 to sp115 etc..

---------

SGI Power Challenge (IRIX64): 

1) machine: davinci.nas.nasa.gov
   login ID: mcinnes
   password: petsc-password
   info: http://lovelace.nas.nasa.gov/Parallel/Cluster/
   info: http://lovelace.nas.nasa.gov/Parallel/Cluster/news.html
   info: http://lovelace.nas.nasa.gov/Parallel/Cluster/faq.html
   
2) NCSA Power Challenge 
   Machine: sif.ncsa.uiuc.edu loki.ncsa.uiuc.edu  odin
   Userid: barrys
   password: petsc-passwd
   Account (whatever that means?) ryc

3) machine:  inca.usi.utah.edu
   login ID:  usilcm
   passwd  :  petsc-passwd
   general info:  http://www.usi.utah.edu 
                  http://usi.utah.edu/user_guides/cug/cug.html
   info on CaseVision/Workshop tools:
          http://usi.utah.edu/user_guides/cug/CUG-10cv.html
   see the directory ~usilcm/work/petsc for latest working
       version of PETSc
   time left in acount:  http://usi.utah.edu/general/allocations.html



---------

ICASE/NASA paragon: Back in action at Va-Tech :(
     machine:  sioux.cc.vt.edu
     account:  mcinnes
     password: petsc-passwd
     info:     http://paradox.cc.vt.edu/ and http://pawne.cc.vt.edu/
     
      - must do "unsetenv TERM" so that make does'nt crash
      - must use make -e PETSC_DIR=<whatever> (which always crashes)
      - for BlockSolve95 muse use 
            make -e BS_DIR=<whatever> PETSC_DIR=<whatever>
      - sample run command:
        pexec "mpirun -np 4 ex8 -log_summary" -sz 4

ICASE/NASA SP2: poseidon.larc.nasa.gov, account: mcinnes
   - info: http://parallel.nas.nasa.gov/Parallel/Metacenter/howto.html
   - report of the year-to-date usage for every group you are a member
     of using the command: acct_ytd 
   - Now jobs submitted to poseidon may actually run on babbage, the
         NASA Ames machine, so that staging input/output files is crucial!
   - Example of script file using staging:
       [poseidon] qsub run.job
     where run.job is a script such as the following:
         
#!/bin/csh
#PBS -l nodes=2
#PBS -l walltime=10:00
#PBS -W stagein=/u/mcinnes/work/euler/ex1@poseidon2.larc:/u/mcinnes/work/runs/ex1
#PBS -W stageout=/u/mcinnes/work/euler/output/o.1@poseidon2.larc:/u/mcinnes/work/runs/out.1
cd /u/mcinnes/work/runs
poe ex1 -mat_baij -optionsleft out.1

    Note: home directories and scratch directoreis are same names on babbage
    and poseidon.

    Note: Sometimes the stagein copies do not work - for no apparent reason. 
    Other times, they work fine.  Workaround:  manually copy input files so
    that no staging is needed.

---------

Paragon account at San Diego 
   telnet paragate.sdsc.edu 7577 ***** cannot log on to it ******
   user id u12461
   cannot ftp to 
   use /usr/local/bin/rftp to get out, you must use numerical
       IP address with rftp
   you must run make with the -e option and have the environmental
   argument PETSC_DIR properly set.


---------
 UCLA Solaris:
  machine : poplar.math.ucla.edu
  user: bsmith
  passwd: petsc-passwd

---------

ICASE suns:
  Ultra2 machines with most memory: gar.icase.edu, croaker.icase.edu

  ete08-f.icase.edu
  elc02.icase.edu
  dolphin.icase.edu

  user: mcinnes
  passwd: petsc-passwd

---------

ODU: solaris,irix, (others too?)
  user: mcinnes
  passwd: odupasswd
  solaris: (Ultra-1) pitfall, dustdevil, wave, dew, rose, orca
---------

Linux accounts:
  machine: ursula.cs.msstate.edu
  user:    curfman
  passwd:  petsc-passwd
  contact:  Gerhard Lehnerer <gerhard@CS.MsState.Edu> (via Tony Skjellum)

  See petsc-maint mail from Igor Markov (UCLA) for another account 
     on his home PC running Linux.

---------

Notre Dame (rs6000, no xlC installed, must use g++ for C++ versions):
  machine: light.ame.nd.edu
  user:    lmcinnes
  passwd:  petsc-passwd

---------

Cornell Theory Center IBM SP2
user: bsmith, balay, curfman
passwd:petsc-passwd

Short jobs log into splogin.tc.cornell.edu limited to 20 cpu minutes
      per 60 minutes
Long jobs splong.tc.cornell.edu limited to 6CPU hours per day


Scheduler use: llsubmit,spsubmit (in ~bsmith/bin)
http://www.tc.cornell.edu/Resources/
spstatus
use kpasswd to change the password

> info from Doug Elias (elias@TC.Cornell.EDU)
> For our installation, you'll almost certainly want to use both
> "-qtune=pwr2" and "-qtune=pwr2s" ... the "pwr2s" directs the
> compiler to tune the executable for running on processors that have
> smaller cache lines (64b, rather than 128 or 256, as i recall), and
> this fits the majority of our nodes. 

Other tc.cornell.edu machines
rs6000: corylus,rhus,populus etc..
IRIX  : thuja,laburnum,juglans

---------
Accounts on NAS machines
For your convenience NAS User Services is available to answer questions
and resolve problems 24 hours each day, 7 days each week.  They can be
reached via telephone at  numbers:(415) 604-4444, FTS 464-4444, (800)
331-8737 or Autovon 359-6771, or via  e-mail at nashelp@nas.nasa.gov.

The message of the day (MOTD) is used at NAS to alert you to problems
or system changes to software or hardware.  Please make note of it each
time you log into the systems.

        Welcome to your lovelace(*) account.

        You will notice that a set of initial "dot" files have
        been added to your home directory.

        Your account has been installed with your current "chuck" password.

        Before you customize these files, please read them carefully.

        Lovelace is a general support system for Parallel Systems.
        You may wish to look on the WWW for Parallel Systems Home Page at:
        http://lovelace.nas.nasa.gov/Parallel/home.html 
        for further information on Parallel Systems.


        (*) This machine is named for Lady Ada Lovelace.

Machines: chuck, scott, lovelace, babbage 
support@nas.nasa.gov user id bfsmith
password initially 2faWquno

----------------------------------------------------------------------
ASCI Blue Mountain at LANL:
  machine:  bluemoutain.acl.lanl.gov (front-end)
  account:  lmcinnes
  password: senmento   (we have a restricted list of password choices)
  info:     http://www.lanl.gov/projects/asci/bluemtn/bluemtn.html
            including details about job submission, etc.

  account : sbalay
  password: smartcard/pin in balays's office

----------------------------------------------------------------------

Yet another Origin2000 (Univ of Utah):
  machines:
     raptor.chpc.utah.edu is the interactive/graphics machine (32 cpus, 4.0 gB memory)
     rapture.chpc.utah.edu  is the batch/compute machine (28 cpus, 3.5 gB memory)
  account: usilcm
  password : petsc-passwd

----------------------------------------------------------------------
MAUI SP2
machine: typhoon.sp2.mhpcc.edu
account: barrys,balay
passwd:  petsc-passwd

info : Useful command showq, showbf, llsubmit

----------------------------------------------------------------------


To kill the running crontab remove bmake/common and bmake/RCS/common,v


Location of email sent to petsc-maint:
  
  Location of old petsc-maint mail is
  /mcs/adm/request/petsc-maint/[active,resolved]
  it may be processed with /usr/local/req/petsc-maint/bin/petsctkreq
  make sure that /usr/local/req/petsc-maint/bin/ is in your path.

FTP log files:
  /nfs/logs/ftp


To determine who is on the PETSc users mailing list send
   to majordomo@mcs.anl.gov "who petsc-users"

To unsubscribe a person from petsc-users use
   unsubscribe petsc-users name

To add an object to the PETSc logging:
  - Add object to plog.h, plog.c, petscview, petscview.cfg, plogmpe.c
    ... anything else?

Delete (or "outdate") range of versions from RCS file:
  rcs -o[r1]:[r2] file, where r1 and r2 denote beginning and ending of range
  (e.g., "rcs -o1.1:1.65 matrix.c")

To run parallel jobs using the /tmp/petsc installations: Use, e.g.,
    mpirun -machinefile $PETSC_DIR/maint/hosts.local -np 2 ex1

To encrypt things for the ftp site, on ryan or violet 
run crypt password < file > coded file. To unencrypt do
crypt password < codeedfile > clear file.

Tips for using complex numbers:
     complex x,y(0,0);
     x = complex(1.1,imag(x));       
     x = complex(real(x),2.1);
     x = complex(1.1,2.2);


--------------------------------------------------------------------------

Lois no at SMU: 214-768-2515

--------------------------------------------------------------------------
Approving mail to petsc-users
The easiest way to approve a message is to create a file ~/.majordomo
that contains the lines:

#list         passwd    site
petsc-users   passwd    majordomo@mcs.anl.gov

You could have multiple lines if you control multple lists.  Then
take the message you have and send it through the program "approve".
For MH based mail, that should be "show|approve".  If using "mail", 
you'd have to save the message to a file then "cat file|approve".
For other mailers, you are on your own to determine how to pipe the
message to a program.  Some can do it, and others are like "mail".

--------------------------------------------------------------------------

Tools for image manipulation (e.g., transparent gifs, animation, interlacing):
 - http://www.vrl.com
 - also, see SGI tool: imgworks  (different capabilities on cyan and violet)

---------------------------------------------------------------------
In HTML, to pop up the page linked to in a different frame, use:
<A HREF="http://www.boeing.com/747.html" TARGET="NEW">Boeing Corporation 747 Fact Sheet</A>
---------------------------------------------------------------------
To restart mpi server on elvis, log into elvis as ~petsc and type serverstart
---------------------------------------------------------------------
on IBMs to use more than 256 Meg of memory, use compiler option   -bmaxdata:0x70000000
This option requests 7 segments, each segment has 256Meg of memory
---------------------------------------------------------------------
To run single proc jobs on quad compile servers do:
>setenv MP_HOSTFILE /homes/balay/hostfile
#where  /homes/balay/hostfile contains the name of the machine where the job is run
>cat /homes/balay/hostfile
ico13
# Then run the executable as any poe job.
>ex5
Number of Newton iterations = 4
>
---------------------------------------------------------------------

  Barry's PPP acount at ect

I have setup ur achilles account for dialup access.
Login name is  bsmith  and password is ZuPiHif
Any questions, call x-5425 or email.

----------------------------------------------------------------------
emacs tags-query-replace example:
\$(\([^\)]*\))
${\1}
replaces all $(something) with ${something}

----------------------------------------------------------------------
useful nm options on various machines

sun4   : nm -o
solairs: nm -A
rs6000 : nm -A
IRIX   : nm -Bo
----------------------------------------------------------------------
SGI Contact for Origin2000:
Sandy Carney <carney@cray.com>
(612) 683-3674
----------------------------------------------------------------------
llnl Gzavf3zh
