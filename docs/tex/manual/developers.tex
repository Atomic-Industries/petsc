% $Id: developers.tex,v 1.1 1998/04/21 23:15:02 bsmith Exp bsmith $ 
%
% LATEX version of the PETSc users manual.
%
% manual_tex.tex is the base file for LaTeX format, while manual.tex is
% the corresponding base HTML format file.
%
\documentstyle[twoside,psfig,../sty/verbatim,../sty/tpage,../sty/here,../sty/anlhelper]{../sty/report_petsc} 
\setlength{\textwidth}{6.5in}
\setlength{\oddsidemargin}{0.0in}
\setlength{\evensidemargin}{0.0in}
\setlength{\textheight}{9.2in}
\setlength{\topmargin}{-.8in}

\newcommand{\findex}[1]{\index{FUNCTION #1}}
\newcommand{\sindex}[1]{\index{#1}}
\newcommand{\F}{\mbox{\boldmath \(F\)}}
\newcommand{\x}{\mbox{\boldmath \(x\)}}
\newcommand{\rr}{\mbox{\boldmath \(r\)}}

\makeindex
 

\begin{document}

\ANLTitle{PETSc 2.0 Developers Manual}{\em Satish Balay\\
William Gropp\\ Lois Curfman McInnes\\ Barry Smith\\
\hspace{0.3in}\\
Mathematics and Computer Science Division \\
http://www.mcs.anl.gov/petsc
\vspace{0.5in} \\
{\rm This manual is intended for use with PETSc 2.0.22}}
{95/11 - Revision 2.0.22}{1998}

\newpage

\hbox{ }

\vspace{1in}
\date{\today}

% Blank page makes double sided printout look bettter.
\newpage

% Abstract for users manual
\addcontentsline{toc}{chapter}{Abstract}

\medskip \medskip



%
%   Next line temp removed
%
\noindent {\bf Abstract:} 

\medskip \medskip
This manual describes how to develop library codes that are compatible and
may be used directly and easily with PETSc 2.0. It assumes that you are 
familar with PETSc, have a copy of the users manual and access to all its
source code and documentation (available via http://www.mcs.anl.gov/petsc).


% $Id: developers.tex,v 1.1 1998/04/21 23:15:02 bsmith Exp bsmith $ 
%
% NOTES:  
%  - Be sure to place captions BEFORE labels in figures and tables!
%    Otherwise, numbering will be incorrect.  For example, use the following:
%       \caption{PETSc Vector Operations}
%       \label{fig:vectorops}
%  - Use \break to indicate a line break (needed to prevent long strings in
%    \tt mode from running of the page)
%


\chapter{Design and Implementations of the  Abstract Classes}
\label{chapter:design}

PETSc 2.0 is designed using strong data encapsulation.  Hence,
any collection of data (for instance, a sparse matrix) is stored in 
a way that is completely private from the application code. The application 
code can manipulate the data only through a well-defined interface, as it 
does {\em not} know how the data is stored internally. 

PETSc is designed around several components (e.g. {\tt Vec} (vectors),
{\tt Mat} (matrices, both dense and sparse)). Each component has
\begin{itemize}
\item Its own include file {\tt \$\{PETSC\_DIR\}/include/<component>.h} 
\item Its own directory, {\tt \$\{PETSC\_DIR\}/src/<component>}
\item An abstract data structure defined in  the file
      {\tt \$\{PETSC\_DIR\}/src/<component>/<component>impl.h}.
      This data structure is shared by all the different implementations of the 
      component. For example, for matrices it is shared by dense,
      sparse, parallel, and sequential formats.
\item An abstract interface that defines the application callable 
      functions for the component. These are defined in the directory
      {\tt \$\{PETSC\_DIR\}/src/<component>/interface}.
\item One or more actual implementations of the components (for example,
      sparse uniprocessor and parallel matrices implemented with the AIJ storage format).
      These are each in a subdirectory of 
      \break {\tt \$\{PETSC\_DIR\}/src/<component>/impls}. Except in rare circumstances data 
      structures defined here should not be referenced from outside this 
      directory.
\end{itemize}

Each type of object, for instance a vector, is defined in its own
include file, by {\tt typedef \_p\_Object* Object;}, (for example, 
{\tt typedef \_p\_Vec* Vec;}).  This organization
allows the compiler to perform type checking while at the same time
completely removing the details of the implementation of {\tt
\_p\_Object} from the application code. The exact details of {\tt
\_p\_Object} may be changed at link time. This capability is extremely important
because it allows the library internals to be changed
without altering or recompiling the application code.

Polymorphism is supported through the directory {\tt \$\{PETSC\_DIR\}/src/<component>/interface},
which contains the code that implements the abstract interface to the
operations on the object.  Essentially, these routines do some error
checking of arguments and logging of profiling information 
and then call the function appropriate for the
particular implementation of the object. The name of the abstract
function is {\tt ObjectOperation}, for instance, {\tt MatMult} or {\tt PCCreate}, while
the name of a particular implementation is 
\break{\tt ObjectOperation\_Implementation}, for instance, 
{\tt MatMult\_SeqAIJ} or {\tt PCCreate\_ILU}. These naming
conventions are used to simplify code maintenance.

Each object structure (named {\tt \_p\_Object}) consists of three parts:
a common PETSc header (defined in {\tt include/petschead.h},
a pointer to a list of operations for the
object, and any additional information that is appropriate for the
particular abstract object.  The PETSc header includes an integer
cookie, an integer type, an MPI communicator, a function pointer that
indicates a destroy routine, and a function pointer that indicates a
viewer routine. The header can also contain additional records.
Several routines are provided for manipulating data within the header,
including
\begin{verbatim}
   int PetscObjectGetComm(PetscObject object,MPI_Comm *comm) 
\end{verbatim}
which returns in {\tt comm} \findex{PetscObjectGetComm()}
\sindex{communicator} the MPI communicator associated with the
specified object.

After the header, each PETSc object contains a pointer to a list of
operations for the object.  For example, the vector operations include
norms, assembly routines, scatters, gathers, etc.  Finally,
information that is appropriate for the particular abstract object is
included. Generally, this information includes a pointer to data used
by a particular implementation.  For example, the {\tt \_p\_Vec}
structure is given by
\begin{verbatim}
   struct _p_Vec {
      PETSCHEADER
      struct _VeOps ops;
      void          *data;
   };
\end{verbatim}

\chapter{Style Guide}

\section{Names}
Consistency of names for variables, functions, etc. is extremely 
important in making the package both usable and maintainable.
We use several conventions:
\begin{itemize}
\item All function names and enum types consist of words, each of 
      which is capitalized, for example {\tt SLESSolve()} and 
      {\tt MatGetReordering()}.
\item All enum elements and macro variables are capitalized. When
      they consist of several
      complete words, there is an underscore between each word.
\item Functions that are private to PETSc (not callable by the 
      application code) either
      \begin{itemize}
        \item have an appended {\tt \_Private} (for example, 
           {\tt StashValues\_Private}) or
        \item have an appended {\tt \_ObjectSubtype} (for example,
           {\tt MatMult\_SeqAIJ}).
      \end{itemize}

      In addition, functions that are not intended for use outside
      of a particular file are declared static.
\item Function names in structures are the same as the base application
      function name without the object prefix, and all are in small letters. 
      For example, {\tt MatMultTrans()} has a structure name of 
      {\tt multtrans()}.
\item Each application usable function begins with the name of the object, 
      for example, {\tt ISInvertPermutation} or {\tt MatMult}. 
\end{itemize}

\section{Coding Conventions and Style Guide}

Within the PETSc source code, we adhere to the following guidelines
so that the code is uniform and easily maintainable:

\begin{itemize}
\item All PETSc function bodies are indented two characters.
\item Each additional level of loops, if statements, etc. is indented
      two more characters.
\item Wrapping lines should be avoided whenever possible.
\item Source code lines should not be more than 120 characters wide.
\item The macros {\tt SETERRQ()} and {\tt CHKERRQ()} should be on the 
      same line as the routine to be checked unless this violates the 
      120 character width rule. Try to make error messages short, but 
      informative.
\item The local variable declarations should be aligned. For example,
      use the style
\begin{verbatim}
   int    i,j;
   Scalar a;
\end{verbatim}
instead of
\begin{verbatim}
   int i,j;
   Scalar a;
\end{verbatim}
\item All local variables of a particular type (e.g., {\tt int}) should be 
      listed on the same line if possible; otherwise, they should be listed
      on adjacent lines.
\item Equal signs should be aligned in regions where possible.
\item There {\em must} be a single blank line
      between the local variable declarations and the body of the function.
\item The first line of the executable statments must be {\tt PetscFunctionBegin;}
\item Use {\tt PetscFunctionReturn(returnvalue);} not {\tt return(returnvalue);}
\item {\em Never} put a function call in a return statment; do not do
\begin{verbatim}
   PetscFunctionReturn( somefunction(...) );
\end{verbatim}
\item Indentation for {\tt if} statements {\em must}  be done  as
as
\begin{verbatim}
   if (  ) {
     ....
   } else {
     ....
   }
\end{verbatim}
\item {\em Never}  have 
\begin{verbatim}
   if (  ) 
     a single indented line
\end{verbatim}
or
\begin{verbatim}
   for (  )
     a single indented line
\end{verbatim}
instead use either 
\begin{verbatim}
   if (  ) a single line
\end{verbatim}
or 
\begin{verbatim}
   if (  ) {
     a single indented line
   }
\end{verbatim}
\item {\em No} tabs are allowed in {\em any} of the source code.
\end{itemize}

\section{Option Names}

Since consistency simplifies usage and code maintenance, the names of
PETSc routines, flags, options, etc. have been selected with great care.
The default option names are of the form {\tt -package\_subpackage\_name}.  
For example, the option name for the basic convergence tolerance for 
the KSP package is {\tt -ksp\_atol}. In addition, operations in different 
packages of a similar nature have a similar name.  For example, the option
name for the basic convergence tolerance for the SNES package is 
{\tt -snes\_atol}.

%- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\chapter{Implementation of Profiling}
\label{sec:profimpl}

This section provides details about the implementation of event
logging and profiling within PETSc.   Chapter~\ref{ch:profiling}
gives information about using the profiling in application codes. 

The interface for profiling in PETSc is contained in the file 
{\tt \$\{PETSC\_DIR\}/include/petsclog.h}. It includes \findex{PLogObjectCreate()}
\begin{verbatim}
   PLogObjectCreate(PetscObject h);
\end{verbatim}
which logs the creation of any PETSc object. This should be included in 
any PETSc source code that uses {\tt PetscHeaderCreate()}. 

Just before an object is destroyed, it is logged
with \findex{PLogObjectDestroy()} 
\begin{verbatim}
   PLogObjectDestroy(PetscObject h);
\end{verbatim}

If an object has a clearly defined parent object (for instance, when 
a work vector is generated for use in a Krylov solver), this information
is logged with the command, \findex{PLogObjectParent()}
\begin{verbatim}
   PLogObjectParent(PetscObject parent,PetscObject child);
\end{verbatim}
It is also useful to log information about the state of an object, as can
be done with the command \findex{PLogObjectState()}
\begin{verbatim}
   #if defined(USE_PETSC_LOG)
   PLogObjectState(PetscObject h,char *format,...);
   #endif
\end{verbatim}
For example, for sparse matrices we usually log the matrix 
dimensions and number of nonzeros.

As discussed in the preceding section, events are logged using the 
pair \findex{PLogEventBegin()}
\begin{verbatim}
   PLogEventBegin(int event,PetscObject o1,PetscObject o2,PetscObject o3,PetscObject o4);
   PLogEventEnd(int event,PetscObject o1,PetscObject o2,PetscObject o3,PetscObject o4);
\end{verbatim}
This logging is usually done in the abstract
interface file for the operations, for example, {\tt src/mat/src/matrix.c}.

Several routines that will be used rarely by the 
application programmer \findex{PLogPrintSummary()}
are \findex{PLogBegin()} \findex{PLogDump()} \findex{PLogAllBegin()} 
\begin{verbatim}
   PLogBegin();
   PLogAllBegin();
   PLogDump(char *filename);
   PLogPrintSummary(FILE *fd);
\end{verbatim}
These routines are normally called by the {\tt PetscInitialize()}
and {\tt PetscFinalize()} routines when the option {\tt -log}, 
{\tt -log\_summary}, or 
{\tt -log\_all} is given.

\chapter{The Various Matrix Classes}
\label{sec:matclasses}

PETSc provides a variety of matrix implementations, since no single
matrix format is appropriate for all problems.  This section first
discusses various matrix blocking strategies, and then briefly
describes the ever-expanding assortment of matrix types within PETSc.

\subsection{Matrix Blocking Strategies}
\sindex{matrix blocking} 
\sindex{blocking} 

In today's computers, the time to perform an arithmetic operation is
dominated by the time to move the data into position, not the time to
compute the arithmetic result.  For example, the time to perform a
multiplication operation may be one clock cycle, while the time to
move the floating point number from memory to the arithmetic unit may
take 10 to 15 cycles. To help manage this difference in time scales,
most processors have at least three levels of memory: registers,
cache, and random access memory, RAM. (In addition, some processors
have external caches, and the complications of paging introduce
another level to the hierarchy.)

Thus, to achieve high performance, a code should first move data into
cache, and from there move it into registers and use it repeatedly
while it remains in the cache or registers before returning it to main
memory. If one reuses a floating point number 50 times while it is in
registers, then the ``hit'' of 10 clock cycles to bring it into the
register is not important. But if the floating point number is used
only once, the ``hit'' of 10 clock cycles becomes very noticeable,
resulting in disappointing flop rates.

Unfortunately, the compiler controls the use of the registers, and the
hardware controls the use of the cache. Since the user has essentially
no direct control, code must be written in such a way that the
compiler and hardware cache system can perform well. Good quality code
is then be said to respect the memory hierarchy.

The standard approach to improving the hardware utilization is to use
``blocking''. That is, rather than working with individual elements in
the matrices, one employs blocks of elements.  Since the use of
implicit methods in PDE-based simulations leads to matrices with a
naturally blocked structure (with a block size equal to the number of
degrees of freedom per cell), blocking is extremely advantageous.  The
PETSc (and BlockSolve95) sparse matrix representations use a variety
of techniques for blocking, including

\begin{itemize}
\item storing the matrices using a generic sparse matrix format, but 
   storing additional information about adjacent rows with identical 
   nonzero structure (so called I-nodes); this I-node information is 
   used in the key computational routines to improve performance
   (the default for the {\tt MATSEQAIJ} and {\tt MATMPIAIJ} formats);
\item storing the matrices using a fixed (problem dependent) block size
   (via the {\tt MATSEQBAIJ} and {\tt MATMPIBAIJ} formats); and
\item storing the matrices using a variable block size, that can be 
   different for different parts of the matrix
   (supported by the BlockSolve95 matrix format {\tt MATMPIROWBS}).
\end{itemize}

The advantage of the first approach is that it is a minimal change
from a standard sparse matrix format and brings a large percent of the
improvement one obtains via blocking.  Using a fixed block size gives
the best performance, since the code can be hardwired with that
particular size (for example, in some problems the size may be 3, in
others 5, etc.), so that the compiler will then optimize for that
size, removing the overhead of small loops entirely. Variable block
size is, of course, appropriate for problems where the natural matrix
block size is different in different parts of the domain. It is
slightly less efficient than the fixed block size code due to overhead
of checking block sizes.

The following table presents the floating point performance
for a basic matrix-vector product using these four approaches: a basic
compressed row storage format (using the PETSc runtime options 
-mat\_seqaij -mat\_no\_unroll); the same compressed row format using
I-nodes (with the option -mat\_seqaij); a fixed block size code,
with a block size of three for these problems (using the option 
-mat\_seqbaij); and the BlockSolve95 variable block size code (using
PETSc option -mat\_mpirowbs). The rates were computed on one
node of an IBM SP2, using two test matrices.  The first matrix
(ARCO1), courtesy of Rick Dean of Arco, arises in multiphase flow
simulation; it has 1501 degrees of freedom, 26,131 matrix nonzeros
and, a natural block size of 3, and a small number of well terms. The
second matrix (CFD), arises in a three-dimensional Euler flow
simulation and has 15,360 degrees of freedom, 496,000 nonzeros, and a
natural block size of 5. In addition to displaying the flop rates for
matrix-vector products, we also display them for triangular solve
obtained from an ILU(0) factorization.

\medskip
\centerline{
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Problem & Block size & Basic & I-node version & Fixed block size &Variable block size \\
\hline
\multicolumn{6}{c}{{\em Matrix-Vector Product (Mflop/sec)}} \\
\hline
Multiphase & 3 & 27 & 43 & 70 & 22 \\
Euler & 5 &  28 & 58 & 90 & 39 \\
\hline
\multicolumn{6}{c}{{\em Triangular Solves from ILU(0) (Mflop/sec)}}\\
\hline
Multiphase & 3 & 22 & 31 & 49 & 15 \\
Euler & 5 & 22 & 39 & 65 & 24\\
\hline
\end{tabular}
}
\medskip

These examples demonstrate that careful implementations of the basic
sequential kernels in PETSc can dramatically improve overall floating
point performance, and users can immediately benefit from such
enhancements without altering a single line of their application
codes.  Note that the speeds of the I-node and fixed block operations
are several times that of the basic sparse implementations.  The
disappointing rates for the variable block size code occur because
even on a sequential computer, the code performs the matrix-vector
products and triangular solves using the coloring introduced above and
thus does not utilize the cache particularly efficiently.  This is an
example of improving the parallelization capability at the expense of
using each processor less efficiently.

\subsection{Sequential AIJ Sparse Matrices}

The default matrix representation within PETSc is the general sparse
AIJ format (also called the Yale sparse matrix format or compressed
sparse row format, CSR). Section~\ref{sec:matsparse} describes this
matrix type.

\subsection{Parallel AIJ Sparse Matrices}

Section~\ref{sec:matsparse} describes this matrix type, which is the
default parallel matrix format; additional implementation details are
given in \cite{efficient}.

\subsection{Sequential Block AIJ Sparse Matrices}

The sequential and parallel block AIJ formats, which are extensions of
the AIJ formats described above, are intended especially for use with
multicomponent PDEs.  The block variants store matrix elements by
fixed-sized dense {\tt nb} $\times$ {\tt nb} blocks, where currently
{\tt nb} ranges from two through five.  These formats are fully
compatible with standard Fortran77 storage.  That is, the stored row
and column indices can begin at either one (as in Fortran) or zero.

The routine for creating a sequential block AIJ matrix with {\tt m} 
rows, {\tt n} columns, and a block size of {\tt nb} is
\begin{verbatim}
   ierr = MatCreateSeqBAIJ(MPI_Comm comm,int nb,int m,int n,int nz,int *nnz, Mat *A)
\end{verbatim}
\findex{MatCreateSeqBAIJ} 
The arguments {\tt nz} and {\tt nnz} can be used to preallocate matrix
memory by indicating the number of {\em block} nonzeros per row.  For good
performance during matrix assembly, preallocation is crucial; however, the
user can set {\tt nz=0} and {\tt nzz=PETSC\_NULL} for PETSc to dynamically
allocate matrix memory as needed.  Section~\ref{sec:matsparse}
discusses preallocation for the AIJ format; extension to the block AIJ
format is straightforward.

Note that the routine {\tt MatSetValuesBlocked()}
\findex{MatSetValuesBlocked()} can be used for more efficient matrix assembly
when using the block AIJ format.
 
\subsection{Parallel Block AIJ Sparse Matrices}

Parallel block AIJ matrices with block size {\t nb} can be created with
the command \findex{MatCreateMPIBAIJ()}
\begin{verbatim}
   ierr = MatCreateMPIBAIJ(MPI_Comm comm,int nb,int m,int n,int M,int N,int d_nz,
                          int *d_nnz, int o_nz,int *o_nnz,Mat *A);
\end{verbatim}
{\tt A} is the newly created matrix, while the arguments {\tt m}, {\tt n}, 
{\tt M}, and {\tt N}, indicate the number of local rows and columns and
the number of global rows and columns, respectively. Either the local or
global parameters can be replaced with {\tt PETSC\_DECIDE}, so that 
PETSc will determine \findex{PETSC_DECIDE} them.
The matrix is stored with a fixed number of rows on 
each processor, given by {\tt m}, or determined by PETSc if {\tt m} is
{\tt PETSC\_DECIDE}.

If {\tt PETSC\_DECIDE} is not used for
{\tt m} and {\tt n} then the user must ensure that they are chosen to be
compatible with the vectors. To do this, one first considers the product 
$y = A x$. The {\tt m} that one uses in {\tt MatCreateMPIBAIJ()}
must match the local size used in the {\tt VecCreateMPI()} for {\tt y}.
The {\tt n} used must match that used as the local size in 
{\tt VecCreateMPI()} for {\tt x}. 

The user must set {\tt d\_nz=0}, {\tt o\_nz=0}, {\tt d\_nnz=PETSC\_NULL}, and 
{\tt o\_nnz=PETSC\_NULL} for PETSc to control dynamic allocation of matrix
memory space.  Analogous to {\tt nz} and {\tt nnz} for the routine 
{\tt MatCreateSeqBAIJ()}, these arguments optionally specify 
block nonzero information for the diagonal ({\tt d\_nz} and {\tt d\_nnz}) and 
off-diagonal ({\tt o\_nz} and {\tt o\_nnz}) parts of the matrix. 
For a square global matrix, we define each processor's diagonal portion 
to be its local rows and the corresponding columns (a square submatrix);  
each processor's off-diagonal portion encompasses the remainder of the
local matrix (a rectangular submatrix).  
Section~\ref{sec:matsparse} gives an example of preallocation for
the parallel AIJ matrix format; extension to the block parallel AIJ case
is straightforward.

\subsection{Sequential Dense Matrices}

PETSc provides both sequential and parallel dense matrix formats,
where each processor stores its entries in a column-major array in the
usual Fortran77 style.  Section~\ref{sec:matdense} provides details
on creating these matrices.

\subsection{Parallel Dense Matrices}

The parallel dense matrices are partitioned by rows across the
processors, so that each local rectangular submatrix is stored in the
dense format described above.

\subsection{Parallel BlockSolve Sparse Matrices}

PETSc provides a parallel, sparse, row-based matrix format that is
intended for use in conjunction with the ILU and ICC preconditioners
in BlockSolve95.  Section~\ref{sec:ilu_icc} gives details on this
matrix type.

\subsection{Block Diagonal Sparse Matrices}
\label{sec:bdiag}

Storage \sindex{block diagonal matrix storage} by block diagonals is
available in both uniprocessor and parallel versions, although currently
only a subset of matrix operations is supported.  Each element of
a block diagonal is defined to be a square dense block of size {\tt
nb} $\times$ {\tt nb}, where conventional diagonal storage results for
{\tt nb}=1.  Such storage is particularly useful for multicomponent PDEs
discretized on regular grids.

The routine for creating a uniprocessor block diagonal matrix with {\tt m} 
rows, {\tt n} columns, and a block size of {\tt nb} is
\begin{verbatim}
   ierr = MatCreateSeqBDiag(PETSC_COMM_SELF,int m,int n,int nd,int nb,int *diag,
                                   Scalar **diagv,Mat *A);
\end{verbatim}
The \findex{MatCreateSeqBDiag} argument {\tt nd} is the number of 
block diagonals, and {\tt diag} is
an array of block diagonal numbers.  For the matrix element $A_{ij}$,
where $i$ and $j$ respectively denote the row and column number of the 
element, the block diagonal number is computed using integer division by
\[ {\tt diag} = i/nb - j/nb. \]
If matrix storage space is allocated by the user, the argument {\tt diagv} 
is a pointer to the actual diagonals (in the same order as the {\tt diag} 
array).  For PETSc to control memory allocation, the user should merely
set {\tt diagv=PETSC\_NULL}.

A simple example of this storage format is illustrated below for block
size {\tt nb}=1. 
Here {\tt nd} = 4 and {\tt diag} = [2, 1, 0, -3]. The
diagonals need not be listed in any particular order, so that
{\tt diag} = [-3, 0, 1, 2] or {\tt diag} = [0, 2, -3, 1] would also
be valid values for the {\tt diag} array. 

\begin{center}
\begin{tabular}{| c c c c c c |}
\hline
a00  &0    &0    &a03  &0    &0\\
a10  &a11  &0    &0    &a14  &0\\
a20  &a21  &a22  &0    &0    &a25\\
0    &a31  &a32  &a33  &0    &0\\
0    &0    &a42  &a43  &a44  &0\\
0    &0    &0    &a53  &a54  &a55\\
\hline
\end{tabular}
\end{center}

\subsection{Parallel Block Diagonal Sparse Matrices}

The parallel block diagonal matrices are partitioned by rows across
the processors, so that each local rectangular submatrix is stored by
block diagonals as described above.  The routine for creating a
parallel block diagonal matrix with {\tt m} local rows, {\tt M} global
rows, {\tt n} global columns, and a block size of {\tt nb} is
\begin{verbatim}
   ierr = MatCreateMPIBDiag(PETSC_COMM_SELF,int m,int M,int N,int nd,int nb,int *diag,
                            Scalar **diagv,Mat *A);
\end{verbatim}
Either the {\tt m} or {\tt M} can be set to {\tt PETSC\_DECIDE} for PETSc
to determine the corresponding quantity.


\bibliography{petsc}

\end{document}
